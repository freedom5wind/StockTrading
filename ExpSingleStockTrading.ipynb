{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import typing\n",
    "from typing import Any, Callable, Dict, Type\n",
    "import warnings\n",
    "\n",
    "from boruta import BorutaPy\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_contour, plot_edf, \\\n",
    "    plot_intermediate_values, plot_optimization_history, plot_parallel_coordinate, \\\n",
    "    plot_param_importances, plot_slice\n",
    "import pandas as pd\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import stockstats\n",
    "import tushare\n",
    "import yfinance as yf\n",
    "\n",
    "from environment.SingleStockTradingEnv import SingleStockTradingEnv\n",
    "from maskable.MaskableDQN import MaskableDQN\n",
    "from maskable.MaskableIQN import MaskableIQN\n",
    "from maskable.MaskableQRDQN import MaskableQRDQN\n",
    "from utils.sample_funcs import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Setup directories\n",
    "DATA_SAVE_DIR = 'datasets'\n",
    "MODEL_DIR = 'models'\n",
    "TENSORBOARD_LOG_DIR = 'tensorboard_log'\n",
    "RAW_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'raw')\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'clean')\n",
    "PREPROCESSED_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'preprocessed')\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, MODEL_DIR, TENSORBOARD_LOG_DIR, \\\n",
    "     RAW_DATA_DIR, CLEAN_DATA_DIR, PREPROCESSED_DATA_DIR])\n",
    "\n",
    "TRAIN_START_DAY = '2008-01-01'\n",
    "TRAIN_END_DAY = '2016-12-31'\n",
    "TEST_START_DAY = '2017-01-01'\n",
    "TEST_END_DAY = '2019-12-31'\n",
    "TRADE_START_DAY = '2020-01-01'\n",
    "TRADE_END_DAY = '2022-12-31'\n",
    "\n",
    "tushare_token = '2bf5fdb105eefda26ef27cc9caa94e6f31ca66e408f7cc54d4fce032'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CSI300-components ticker list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csi300_component_ticker_list(url: str, download_dir: str) -> List[str]:\n",
    "    download_file_name = url.split('/')[-1]\n",
    "    download_file_path = os.path.join(download_dir, download_file_name)\n",
    "    if not os.path.exists(download_file_path):\n",
    "        r =requests.get(url)\n",
    "        with open(download_file_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            f.close()\n",
    "    df = pd.read_excel(download_file_path)\n",
    "    df.loc[df['交易所Exchange'] == '上海证券交易所', 'suffix'] = 'SS'\n",
    "    df.loc[df['交易所Exchange'] == '深圳证券交易所', 'suffix'] = 'SZ'\n",
    "    tic_list = [f'{code:06d}.{suffix}' for code, suffix in zip(df['成分券代码Constituent Code'], df['suffix'])]\n",
    "    return tic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://csi-web-dev.oss-cn-shanghai-finance-1-pub.aliyuncs.com/static/html/csindex/public/uploads/file/autofile/cons/000300cons.xls'\n",
    "tic_list = download_csi300_component_ticker_list(url, DATA_SAVE_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CSI300 tickers with yfinace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_with_yfince(tic_list: List[str], download_dir: str) -> List[str]:\n",
    "    retry_list = []\n",
    "    for tic in tic_list:\n",
    "        csv_path = os.path.join(download_dir, f'{tic}.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f'File {csv_path} already exist. Skip')\n",
    "            continue\n",
    "        \n",
    "        ticker = yf.Ticker(tic)\n",
    "        df = ticker.history(period='max')\n",
    "        if df.shape[0] > 0:\n",
    "            df.to_csv(csv_path)\n",
    "            print(f'Download {tic}.csv')         \n",
    "            time.sleep(0.1)\n",
    "        else:\n",
    "            retry_list.append(tic)\n",
    "    \n",
    "    return retry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 300601.SZ.csv\n",
      "Download 300628.SZ.csv\n",
      "Download 603659.SS.csv\n",
      "Failed to get ticker '002916.SZ' reason: HTTPSConnectionPool(host='query2.finance.yahoo.com', port=443): Max retries exceeded with url: /v8/finance/chart/002916.SZ?range=1d&interval=1d (Caused by ProxyError('Cannot connect to proxy.', timeout('_ssl.c:1112: The handshake operation timed out')))\n",
      "- 002916.SZ: No timezone found, symbol may be delisted\n",
      "Download 002916.SZ.csv\n",
      "600036.SS: No data found for this date range, symbol may be delisted\n",
      "Download 600036.SS.csv\n",
      "Download 000776.SZ.csv\n",
      "Download 600089.SS.csv\n",
      "Download 600884.SS.csv\n",
      "Download 600085.SS.csv\n",
      "Download 601360.SS.csv\n",
      "Download 603259.SS.csv\n",
      "Download 300454.SZ.csv\n",
      "Download 601066.SS.csv\n",
      "Download 300760.SZ.csv\n",
      "Download 300751.SZ.csv\n",
      "Download 601838.SS.csv\n",
      "Download 000651.SZ.csv\n",
      "Download 000661.SZ.csv\n",
      "Download 000733.SZ.csv\n",
      "Download 000858.SZ.csv\n",
      "Download 002050.SZ.csv\n",
      "Download 600048.SS.csv\n",
      "Download 002236.SZ.csv\n",
      "Download 601111.SS.csv\n",
      "Download 300763.SZ.csv\n",
      "Download 600989.SS.csv\n",
      "Download 300782.SZ.csv\n",
      "Download 601698.SS.csv\n",
      "Download 601236.SS.csv\n",
      "Download 002064.SZ.csv\n",
      "Download 000876.SZ.csv\n",
      "Download 601088.SS.csv\n",
      "Download 601628.SS.csv\n",
      "Download 601988.SS.csv\n",
      "Download 688005.SS.csv\n",
      "Download 003816.SZ.csv\n",
      "Download 688036.SS.csv\n",
      "Download 688363.SS.csv\n",
      "Download 688111.SS.csv\n",
      "Download 601658.SS.csv\n",
      "Download 601816.SS.csv\n",
      "Download 688169.SS.csv\n",
      "Download 688396.SS.csv\n",
      "Download 000786.SZ.csv\n",
      "Download 000723.SZ.csv\n",
      "Download 600745.SS.csv\n",
      "Download 002008.SZ.csv\n",
      "Download 002007.SZ.csv\n",
      "Download 002049.SZ.csv\n",
      "Download 601898.SS.csv\n",
      "Download 601919.SS.csv\n",
      "Download 300122.SZ.csv\n",
      "Download 300124.SZ.csv\n",
      "Download 600183.SS.csv\n",
      "Download 600009.SS.csv\n",
      "Download 601998.SS.csv\n",
      "Download 601600.SS.csv\n",
      "Download 600196.SS.csv\n",
      "Download 600426.SS.csv\n",
      "Download 601899.SS.csv\n",
      "Download 002493.SZ.csv\n",
      "Download 300979.SZ.csv\n",
      "Download 688303.SS.csv\n",
      "Download 688187.SS.csv\n",
      "Download 601868.SS.csv\n",
      "Download 600941.SS.csv\n",
      "Download 300014.SZ.csv\n",
      "Download 600150.SS.csv\n",
      "Download 600010.SS.csv\n",
      "Download 600690.SS.csv\n",
      "Download 601166.SS.csv\n",
      "Download 600999.SS.csv\n",
      "Download 601390.SS.csv\n",
      "Download 600837.SS.csv\n",
      "Download 600132.SS.csv\n",
      "Download 600115.SS.csv\n",
      "Download 600104.SS.csv\n",
      "Download 600031.SS.csv\n",
      "Download 002202.SZ.csv\n",
      "Download 300059.SZ.csv\n",
      "Download 600845.SS.csv\n",
      "Download 002352.SZ.csv\n",
      "Download 601857.SS.csv\n",
      "Download 601939.SS.csv\n",
      "Download 002129.SZ.csv\n",
      "Download 600918.SS.csv\n",
      "Download 688599.SS.csv\n",
      "Download 688981.SS.csv\n",
      "Download 688561.SS.csv\n",
      "Download 300896.SZ.csv\n",
      "Download 300999.SZ.csv\n",
      "Download 300919.SZ.csv\n",
      "Download 600406.SS.csv\n",
      "Download 600061.SS.csv\n",
      "Download 600309.SS.csv\n",
      "Download 600460.SS.csv\n",
      "Download 002311.SZ.csv\n",
      "Download 000938.SZ.csv\n",
      "Download 600436.SS.csv\n",
      "Download 600029.SS.csv\n",
      "Download 601336.SS.csv\n",
      "Download 300207.SZ.csv\n",
      "Download 002555.SZ.csv\n",
      "Download 002594.SZ.csv\n",
      "Download 002602.SZ.csv\n",
      "Download 601901.SS.csv\n",
      "Download 002601.SZ.csv\n",
      "Download 000596.SZ.csv\n",
      "Download 300316.SZ.csv\n",
      "Download 601633.SS.csv\n",
      "Download 601669.SS.csv\n",
      "Download 000408.SZ.csv\n",
      "Download 000001.SZ.csv\n",
      "Download 000002.SZ.csv\n",
      "Download 000301.SZ.csv\n",
      "Download 000538.SZ.csv\n",
      "Download 000568.SZ.csv\n",
      "Download 601012.SS.csv\n",
      "Download 601238.SS.csv\n",
      "Download 601800.SS.csv\n",
      "Download 300347.SZ.csv\n",
      "Download 603993.SS.csv\n",
      "Download 000333.SZ.csv\n",
      "Download 603806.SS.csv\n",
      "Download 603899.SS.csv\n",
      "Download 603799.SS.csv\n",
      "Download 601689.SS.csv\n",
      "Download 600958.SS.csv\n",
      "Download 002756.SZ.csv\n",
      "Download 601985.SS.csv\n",
      "Download 300529.SZ.csv\n",
      "Download 603986.SS.csv\n",
      "Download 601229.SS.csv\n",
      "Download 300595.SZ.csv\n",
      "Download 002841.SZ.csv\n",
      "Download 601881.SS.csv\n",
      "Download 603833.SS.csv\n",
      "Download 603501.SS.csv\n",
      "Download 300661.SZ.csv\n",
      "Download 601878.SS.csv\n",
      "Download 603882.SS.csv\n",
      "Download 603260.SS.csv\n",
      "Download 600025.SS.csv\n",
      "Download 002920.SZ.csv\n",
      "Download 600585.SS.csv\n",
      "Download 000768.SZ.csv\n",
      "Download 000792.SZ.csv\n",
      "Download 600519.SS.csv\n",
      "Download 600887.SS.csv\n",
      "Download 600893.SS.csv\n",
      "Download 600050.SS.csv\n",
      "Download 603486.SS.csv\n",
      "Download 601138.SS.csv\n",
      "Download 300750.SZ.csv\n",
      "Download 002938.SZ.csv\n",
      "Download 601319.SS.csv\n",
      "Download 300142.SZ.csv\n",
      "Download 000708.SZ.csv\n",
      "Download 002027.SZ.csv\n",
      "Download 002032.SZ.csv\n",
      "Download 601398.SS.csv\n",
      "Download 603185.SS.csv\n",
      "Download 601615.SS.csv\n",
      "Download 300759.SZ.csv\n",
      "Download 601865.SS.csv\n",
      "Download 300769.SZ.csv\n",
      "Download 688008.SS.csv\n",
      "Download 688012.SS.csv\n",
      "Download 002271.SZ.csv\n",
      "Download 002475.SZ.csv\n",
      "Download 601766.SS.csv\n",
      "Download 601668.SS.csv\n",
      "Download 002304.SZ.csv\n",
      "Download 601688.SS.csv\n",
      "Download 000100.SZ.csv\n",
      "Download 600438.SS.csv\n",
      "Download 603290.SS.csv\n",
      "Download 603195.SS.csv\n",
      "Download 601288.SS.csv\n",
      "Download 002001.SZ.csv\n",
      "Download 601117.SS.csv\n",
      "Download 601808.SS.csv\n",
      "Download 600018.SS.csv\n",
      "Download 600763.SS.csv\n",
      "Download 600383.SS.csv\n",
      "Download 600588.SS.csv\n",
      "Download 600019.SS.csv\n",
      "Download 600016.SS.csv\n",
      "Download 600011.SS.csv\n",
      "Download 600674.SS.csv\n",
      "Download 002459.SZ.csv\n",
      "Download 002460.SZ.csv\n",
      "Download 000338.SZ.csv\n",
      "Download 600276.SS.csv\n",
      "Download 600000.SS.csv\n",
      "Download 002466.SZ.csv\n",
      "Download 601888.SS.csv\n",
      "Download 002410.SZ.csv\n",
      "Download 002180.SZ.csv\n",
      "Download 605117.SS.csv\n",
      "Download 605499.SS.csv\n",
      "Download 600905.SS.csv\n",
      "Download 601728.SS.csv\n",
      "Download 001289.SZ.csv\n",
      "Download 300015.SZ.csv\n",
      "Download 600188.SS.csv\n",
      "Download 600039.SS.csv\n",
      "Download 600900.SS.csv\n",
      "Download 600332.SS.csv\n",
      "Download 600176.SS.csv\n",
      "Download 600600.SS.csv\n",
      "Download 600028.SS.csv\n",
      "Download 600346.SS.csv\n",
      "Download 601877.SS.csv\n",
      "Download 002230.SZ.csv\n",
      "Download 601601.SS.csv\n",
      "Download 002414.SZ.csv\n",
      "Download 002179.SZ.csv\n",
      "Download 601788.SS.csv\n",
      "Download 002142.SZ.csv\n",
      "Download 601009.SS.csv\n",
      "Download 002074.SZ.csv\n",
      "Download 600606.SS.csv\n",
      "Download 600660.SS.csv\n",
      "Download 601618.SS.csv\n",
      "Download 600219.SS.csv\n",
      "Download 000977.SZ.csv\n",
      "Download 002120.SZ.csv\n",
      "Download 601818.SS.csv\n",
      "Download 600741.SS.csv\n",
      "Download 600886.SS.csv\n",
      "Download 002252.SZ.csv\n",
      "Download 601989.SS.csv\n",
      "Download 300033.SZ.csv\n",
      "Download 601186.SS.csv\n",
      "Download 601318.SS.csv\n",
      "Download 601006.SS.csv\n",
      "Download 688126.SS.csv\n",
      "Download 603392.SS.csv\n",
      "Download 688065.SS.csv\n",
      "Download 601995.SS.csv\n",
      "Download 300957.SZ.csv\n",
      "Download 600760.SS.csv\n",
      "Download 600795.SS.csv\n",
      "Download 600111.SS.csv\n",
      "Download 600809.SS.csv\n",
      "Download 600030.SS.csv\n",
      "Download 600803.SS.csv\n",
      "Download 600362.SS.csv\n",
      "Download 600233.SS.csv\n",
      "Download 002415.SZ.csv\n",
      "Download 601377.SS.csv\n",
      "Download 000800.SZ.csv\n",
      "Download 002371.SZ.csv\n",
      "Download 002241.SZ.csv\n",
      "Download 601328.SS.csv\n",
      "Download 601169.SS.csv\n",
      "Download 000877.SZ.csv\n",
      "Download 000895.SZ.csv\n",
      "Download 600547.SS.csv\n",
      "Download 600015.SS.csv\n",
      "Download 600584.SS.csv\n",
      "Download 601799.SS.csv\n",
      "Download 000963.SZ.csv\n",
      "Download 600570.SS.csv\n",
      "Download 000425.SZ.csv\n",
      "Download 300223.SZ.csv\n",
      "Download 002600.SZ.csv\n",
      "Download 601216.SS.csv\n",
      "Download 601100.SS.csv\n",
      "Download 300274.SZ.csv\n",
      "Download 000063.SZ.csv\n",
      "Download 000069.SZ.csv\n",
      "Download 000157.SZ.csv\n",
      "Download 000625.SZ.csv\n",
      "Download 000725.SZ.csv\n",
      "Download 002648.SZ.csv\n",
      "Download 002709.SZ.csv\n",
      "Download 002714.SZ.csv\n",
      "Download 601225.SS.csv\n",
      "Download 603288.SS.csv\n",
      "Download 603369.SS.csv\n",
      "Download 603019.SS.csv\n",
      "Download 300408.SZ.csv\n",
      "Download 002736.SZ.csv\n",
      "Download 300413.SZ.csv\n",
      "Download 601021.SS.csv\n",
      "Download 000166.SZ.csv\n",
      "Download 300433.SZ.csv\n",
      "Failed to get ticker '300450.SZ' reason: HTTPSConnectionPool(host='query2.finance.yahoo.com', port=443): Read timed out. (read timeout=10)\n",
      "- 300450.SZ: No timezone found, symbol may be delisted\n",
      "Download 300450.SZ.csv\n",
      "Download 601211.SS.csv\n",
      "Download 300498.SZ.csv\n",
      "Download 601155.SS.csv\n",
      "Download 300496.SZ.csv\n",
      "Download 001979.SZ.csv\n",
      "Download 601966.SS.csv\n",
      "Download 600919.SS.csv\n",
      "Download 002812.SZ.csv\n",
      "Download 600926.SS.csv\n",
      "Download 002821.SZ.csv\n"
     ]
    }
   ],
   "source": [
    "retry_list = download_ticker_with_yfince(tic_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_list = download_ticker_with_yfince(retry_list, RAW_DATA_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calendar_with_tushare(start: str, end: str) -> pd.Series:\n",
    "    start = start.replace('-', '')\n",
    "    end = end.replace('-', '')\n",
    "\n",
    "    tushare.set_token(tushare_token)\n",
    "    tu_pro = tushare.pro_api()\n",
    "    calendar_ss = tu_pro.trade_cal(exchange='SSE', start_date=start, end_date=end, is_open=1)\n",
    "    calendar_sz = tu_pro.trade_cal(exchange='SZSE', start_date=start, end_date=end, is_open=1)\n",
    "    if calendar_ss.shape[0] != calendar_ss.shape[0]:\n",
    "        calendar = pd.merge(calendar_ss.cal_date, calendar_sz.cal_date, on=['cal_date'], how='outer')\n",
    "    else:\n",
    "        calendar = calendar_ss.cal_date\n",
    "\n",
    "    calendar = pd.to_datetime(calendar, format='%Y%m%d')\n",
    "    calendar.rename('date', inplace=True)\n",
    "    \n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_from_yfinance(data: pd.DataFrame, calendar: pd.Series = None) -> pd.DataFrame:\n",
    "    # TODO: calculate adjusted price.\n",
    "    data.drop(labels=['Dividends', 'Stock Splits'], axis='columns', inplace=True)\n",
    "    data.rename(columns={\n",
    "        'Date': 'date',\n",
    "        'Open': 'open',\n",
    "        'High': 'high',\n",
    "        'Low': 'low',\n",
    "        'Close' : 'close',\n",
    "        'Volume' : 'volume'\n",
    "        }, inplace=True)\n",
    "    data['date'] = pd.to_datetime(data['date'].apply(lambda s: s.split(' ')[0]), format='%Y-%m-%d')\n",
    "    data = pd.merge(calendar, data, how='left', on='date')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 rows droped from 000001.SZ.csv.\n",
      "151 rows droped from 000002.SZ.csv.\n",
      "81 rows droped from 000063.SZ.csv.\n",
      "66 rows droped from 000069.SZ.csv.\n",
      "207 rows droped from 000100.SZ.csv.\n",
      "64 rows droped from 000157.SZ.csv.\n",
      "255 rows droped from 000166.SZ.csv.\n",
      "179 rows droped from 000301.SZ.csv.\n",
      "1445 rows droped from 000333.SZ.csv.\n",
      "Too many NaN for data in 000333.SZ.csv.\n",
      "44 rows droped from 000338.SZ.csv.\n",
      "389 rows droped from 000408.SZ.csv.\n",
      "Too many NaN for data in 000408.SZ.csv.\n",
      "83 rows droped from 000425.SZ.csv.\n",
      "170 rows droped from 000538.SZ.csv.\n",
      "33 rows droped from 000568.SZ.csv.\n",
      "9 rows droped from 000596.SZ.csv.\n",
      "176 rows droped from 000625.SZ.csv.\n",
      "165 rows droped from 000651.SZ.csv.\n",
      "39 rows droped from 000661.SZ.csv.\n",
      "109 rows droped from 000708.SZ.csv.\n",
      "343 rows droped from 000723.SZ.csv.\n",
      "21 rows droped from 000725.SZ.csv.\n",
      "34 rows droped from 000733.SZ.csv.\n",
      "60 rows droped from 000768.SZ.csv.\n",
      "534 rows droped from 000776.SZ.csv.\n",
      "Too many NaN for data in 000776.SZ.csv.\n",
      "150 rows droped from 000786.SZ.csv.\n",
      "516 rows droped from 000792.SZ.csv.\n",
      "Too many NaN for data in 000792.SZ.csv.\n",
      "34 rows droped from 000800.SZ.csv.\n",
      "80 rows droped from 000858.SZ.csv.\n",
      "190 rows droped from 000876.SZ.csv.\n",
      "38 rows droped from 000877.SZ.csv.\n",
      "211 rows droped from 000895.SZ.csv.\n",
      "199 rows droped from 000938.SZ.csv.\n",
      "42 rows droped from 000963.SZ.csv.\n",
      "67 rows droped from 000977.SZ.csv.\n",
      "3421 rows droped from 001289.SZ.csv.\n",
      "Too many NaN for data in 001289.SZ.csv.\n",
      "205 rows droped from 001979.SZ.csv.\n",
      "15 rows droped from 002001.SZ.csv.\n",
      "11 rows droped from 002007.SZ.csv.\n",
      "26 rows droped from 002008.SZ.csv.\n",
      "92 rows droped from 002027.SZ.csv.\n",
      "48 rows droped from 002032.SZ.csv.\n",
      "278 rows droped from 002049.SZ.csv.\n",
      "115 rows droped from 002050.SZ.csv.\n",
      "18 rows droped from 002064.SZ.csv.\n",
      "214 rows droped from 002074.SZ.csv.\n",
      "113 rows droped from 002120.SZ.csv.\n",
      "464 rows droped from 002129.SZ.csv.\n",
      "Too many NaN for data in 002129.SZ.csv.\n",
      "9 rows droped from 002142.SZ.csv.\n",
      "11 rows droped from 002179.SZ.csv.\n",
      "326 rows droped from 002180.SZ.csv.\n",
      "15 rows droped from 002202.SZ.csv.\n",
      "155 rows droped from 002230.SZ.csv.\n",
      "105 rows droped from 002236.SZ.csv.\n",
      "109 rows droped from 002241.SZ.csv.\n",
      "581 rows droped from 002252.SZ.csv.\n",
      "Too many NaN for data in 002252.SZ.csv.\n",
      "182 rows droped from 002271.SZ.csv.\n",
      "452 rows droped from 002304.SZ.csv.\n",
      "Too many NaN for data in 002304.SZ.csv.\n",
      "475 rows droped from 002311.SZ.csv.\n",
      "Too many NaN for data in 002311.SZ.csv.\n",
      "563 rows droped from 002352.SZ.csv.\n",
      "Too many NaN for data in 002352.SZ.csv.\n",
      "614 rows droped from 002371.SZ.csv.\n",
      "Too many NaN for data in 002371.SZ.csv.\n",
      "621 rows droped from 002410.SZ.csv.\n",
      "Too many NaN for data in 002410.SZ.csv.\n",
      "706 rows droped from 002414.SZ.csv.\n",
      "Too many NaN for data in 002414.SZ.csv.\n",
      "766 rows droped from 002415.SZ.csv.\n",
      "Too many NaN for data in 002415.SZ.csv.\n",
      "720 rows droped from 002459.SZ.csv.\n",
      "Too many NaN for data in 002459.SZ.csv.\n",
      "715 rows droped from 002460.SZ.csv.\n",
      "Too many NaN for data in 002460.SZ.csv.\n",
      "798 rows droped from 002466.SZ.csv.\n",
      "Too many NaN for data in 002466.SZ.csv.\n",
      "686 rows droped from 002475.SZ.csv.\n",
      "Too many NaN for data in 002475.SZ.csv.\n",
      "2542 rows droped from 002493.SZ.csv.\n",
      "Too many NaN for data in 002493.SZ.csv.\n",
      "1039 rows droped from 002555.SZ.csv.\n",
      "Too many NaN for data in 002555.SZ.csv.\n",
      "861 rows droped from 002594.SZ.csv.\n",
      "Too many NaN for data in 002594.SZ.csv.\n",
      "1106 rows droped from 002600.SZ.csv.\n",
      "Too many NaN for data in 002600.SZ.csv.\n",
      "958 rows droped from 002601.SZ.csv.\n",
      "Too many NaN for data in 002601.SZ.csv.\n",
      "1333 rows droped from 002602.SZ.csv.\n",
      "Too many NaN for data in 002602.SZ.csv.\n",
      "995 rows droped from 002648.SZ.csv.\n",
      "Too many NaN for data in 002648.SZ.csv.\n",
      "1497 rows droped from 002709.SZ.csv.\n",
      "Too many NaN for data in 002709.SZ.csv.\n",
      "1500 rows droped from 002714.SZ.csv.\n",
      "Too many NaN for data in 002714.SZ.csv.\n",
      "1707 rows droped from 002736.SZ.csv.\n",
      "Too many NaN for data in 002736.SZ.csv.\n",
      "1923 rows droped from 002756.SZ.csv.\n",
      "Too many NaN for data in 002756.SZ.csv.\n",
      "2204 rows droped from 002812.SZ.csv.\n",
      "Too many NaN for data in 002812.SZ.csv.\n",
      "2163 rows droped from 002821.SZ.csv.\n",
      "Too many NaN for data in 002821.SZ.csv.\n",
      "2205 rows droped from 002841.SZ.csv.\n",
      "Too many NaN for data in 002841.SZ.csv.\n",
      "2424 rows droped from 002916.SZ.csv.\n",
      "Too many NaN for data in 002916.SZ.csv.\n",
      "2432 rows droped from 002920.SZ.csv.\n",
      "Too many NaN for data in 002920.SZ.csv.\n",
      "2617 rows droped from 002938.SZ.csv.\n",
      "Too many NaN for data in 002938.SZ.csv.\n",
      "2835 rows droped from 003816.SZ.csv.\n",
      "Too many NaN for data in 003816.SZ.csv.\n",
      "495 rows droped from 300014.SZ.csv.\n",
      "Too many NaN for data in 300014.SZ.csv.\n",
      "461 rows droped from 300015.SZ.csv.\n",
      "Too many NaN for data in 300015.SZ.csv.\n",
      "502 rows droped from 300033.SZ.csv.\n",
      "Too many NaN for data in 300033.SZ.csv.\n",
      "593 rows droped from 300059.SZ.csv.\n",
      "Too many NaN for data in 300059.SZ.csv.\n",
      "739 rows droped from 300122.SZ.csv.\n",
      "Too many NaN for data in 300122.SZ.csv.\n",
      "683 rows droped from 300124.SZ.csv.\n",
      "Too many NaN for data in 300124.SZ.csv.\n",
      "963 rows droped from 300142.SZ.csv.\n",
      "Too many NaN for data in 300142.SZ.csv.\n",
      "850 rows droped from 300207.SZ.csv.\n",
      "Too many NaN for data in 300207.SZ.csv.\n",
      "1070 rows droped from 300223.SZ.csv.\n",
      "Too many NaN for data in 300223.SZ.csv.\n",
      "954 rows droped from 300274.SZ.csv.\n",
      "Too many NaN for data in 300274.SZ.csv.\n",
      "1157 rows droped from 300316.SZ.csv.\n",
      "Too many NaN for data in 300316.SZ.csv.\n",
      "1259 rows droped from 300347.SZ.csv.\n",
      "Too many NaN for data in 300347.SZ.csv.\n",
      "1692 rows droped from 300408.SZ.csv.\n",
      "Too many NaN for data in 300408.SZ.csv.\n",
      "1970 rows droped from 300413.SZ.csv.\n",
      "Too many NaN for data in 300413.SZ.csv.\n",
      "1773 rows droped from 300433.SZ.csv.\n",
      "Too many NaN for data in 300433.SZ.csv.\n",
      "1937 rows droped from 300450.SZ.csv.\n",
      "Too many NaN for data in 300450.SZ.csv.\n",
      "2523 rows droped from 300454.SZ.csv.\n",
      "Too many NaN for data in 300454.SZ.csv.\n",
      "1993 rows droped from 300496.SZ.csv.\n",
      "Too many NaN for data in 300496.SZ.csv.\n",
      "1914 rows droped from 300498.SZ.csv.\n",
      "Too many NaN for data in 300498.SZ.csv.\n",
      "2090 rows droped from 300529.SZ.csv.\n",
      "Too many NaN for data in 300529.SZ.csv.\n",
      "2203 rows droped from 300595.SZ.csv.\n",
      "Too many NaN for data in 300595.SZ.csv.\n",
      "2213 rows droped from 300601.SZ.csv.\n",
      "Too many NaN for data in 300601.SZ.csv.\n",
      "2240 rows droped from 300628.SZ.csv.\n",
      "Too many NaN for data in 300628.SZ.csv.\n",
      "2301 rows droped from 300661.SZ.csv.\n",
      "Too many NaN for data in 300661.SZ.csv.\n",
      "2541 rows droped from 300750.SZ.csv.\n",
      "Too many NaN for data in 300750.SZ.csv.\n",
      "2643 rows droped from 300751.SZ.csv.\n",
      "Too many NaN for data in 300751.SZ.csv.\n",
      "2697 rows droped from 300759.SZ.csv.\n",
      "Too many NaN for data in 300759.SZ.csv.\n",
      "2625 rows droped from 300760.SZ.csv.\n",
      "Too many NaN for data in 300760.SZ.csv.\n",
      "2728 rows droped from 300763.SZ.csv.\n",
      "Too many NaN for data in 300763.SZ.csv.\n",
      "2746 rows droped from 300769.SZ.csv.\n",
      "Too many NaN for data in 300769.SZ.csv.\n",
      "2786 rows droped from 300782.SZ.csv.\n",
      "Too many NaN for data in 300782.SZ.csv.\n",
      "3101 rows droped from 300896.SZ.csv.\n",
      "Too many NaN for data in 300896.SZ.csv.\n",
      "3157 rows droped from 300919.SZ.csv.\n",
      "Too many NaN for data in 300919.SZ.csv.\n",
      "3217 rows droped from 300957.SZ.csv.\n",
      "Too many NaN for data in 300957.SZ.csv.\n",
      "3238 rows droped from 300979.SZ.csv.\n",
      "Too many NaN for data in 300979.SZ.csv.\n",
      "3108 rows droped from 300999.SZ.csv.\n",
      "Too many NaN for data in 300999.SZ.csv.\n",
      "45 rows droped from 600000.SS.csv.\n",
      "14 rows droped from 600009.SS.csv.\n",
      "73 rows droped from 600010.SS.csv.\n",
      "17 rows droped from 600011.SS.csv.\n",
      "16 rows droped from 600015.SS.csv.\n",
      "2 rows droped from 600016.SS.csv.\n",
      "63 rows droped from 600018.SS.csv.\n",
      "99 rows droped from 600019.SS.csv.\n",
      "2431 rows droped from 600025.SS.csv.\n",
      "Too many NaN for data in 600025.SS.csv.\n",
      "7 rows droped from 600028.SS.csv.\n",
      "36 rows droped from 600029.SS.csv.\n",
      "31 rows droped from 600030.SS.csv.\n",
      "17 rows droped from 600031.SS.csv.\n",
      "22 rows droped from 600036.SS.csv.\n",
      "79 rows droped from 600039.SS.csv.\n",
      "9 rows droped from 600048.SS.csv.\n",
      "107 rows droped from 600050.SS.csv.\n",
      "130 rows droped from 600061.SS.csv.\n",
      "4 rows droped from 600085.SS.csv.\n",
      "33 rows droped from 600089.SS.csv.\n",
      "66 rows droped from 600104.SS.csv.\n",
      "8 rows droped from 600111.SS.csv.\n",
      "71 rows droped from 600115.SS.csv.\n",
      "44 rows droped from 600132.SS.csv.\n",
      "150 rows droped from 600150.SS.csv.\n",
      "45 rows droped from 600176.SS.csv.\n",
      "10 rows droped from 600183.SS.csv.\n",
      "13 rows droped from 600188.SS.csv.\n",
      "20 rows droped from 600196.SS.csv.\n",
      "107 rows droped from 600219.SS.csv.\n",
      "97 rows droped from 600233.SS.csv.\n",
      "5 rows droped from 600276.SS.csv.\n",
      "136 rows droped from 600309.SS.csv.\n",
      "181 rows droped from 600332.SS.csv.\n",
      "303 rows droped from 600346.SS.csv.\n",
      "19 rows droped from 600362.SS.csv.\n",
      "14 rows droped from 600383.SS.csv.\n",
      "142 rows droped from 600406.SS.csv.\n",
      "12 rows droped from 600426.SS.csv.\n",
      "20 rows droped from 600436.SS.csv.\n",
      "159 rows droped from 600438.SS.csv.\n",
      "93 rows droped from 600460.SS.csv.\n",
      "5 rows droped from 600519.SS.csv.\n",
      "185 rows droped from 600547.SS.csv.\n",
      "57 rows droped from 600570.SS.csv.\n",
      "224 rows droped from 600584.SS.csv.\n",
      "16 rows droped from 600585.SS.csv.\n",
      "18 rows droped from 600588.SS.csv.\n",
      "9 rows droped from 600600.SS.csv.\n",
      "330 rows droped from 600606.SS.csv.\n",
      "3 rows droped from 600660.SS.csv.\n",
      "61 rows droped from 600674.SS.csv.\n",
      "96 rows droped from 600690.SS.csv.\n",
      "98 rows droped from 600741.SS.csv.\n",
      "607 rows droped from 600745.SS.csv.\n",
      "Too many NaN for data in 600745.SS.csv.\n",
      "170 rows droped from 600760.SS.csv.\n",
      "164 rows droped from 600763.SS.csv.\n",
      "95 rows droped from 600795.SS.csv.\n",
      "187 rows droped from 600803.SS.csv.\n",
      "14 rows droped from 600809.SS.csv.\n",
      "14 rows droped from 600837.SS.csv.\n",
      "63 rows droped from 600845.SS.csv.\n",
      "23 rows droped from 600884.SS.csv.\n",
      "133 rows droped from 600886.SS.csv.\n",
      "36 rows droped from 600887.SS.csv.\n",
      "372 rows droped from 600893.SS.csv.\n",
      "Too many NaN for data in 600893.SS.csv.\n",
      "368 rows droped from 600900.SS.csv.\n",
      "Too many NaN for data in 600900.SS.csv.\n",
      "3268 rows droped from 600905.SS.csv.\n",
      "Too many NaN for data in 600905.SS.csv.\n",
      "3020 rows droped from 600918.SS.csv.\n",
      "Too many NaN for data in 600918.SS.csv.\n",
      "2096 rows droped from 600919.SS.csv.\n",
      "Too many NaN for data in 600919.SS.csv.\n",
      "2145 rows droped from 600926.SS.csv.\n",
      "Too many NaN for data in 600926.SS.csv.\n",
      "3408 rows droped from 600941.SS.csv.\n",
      "Too many NaN for data in 600941.SS.csv.\n",
      "1764 rows droped from 600958.SS.csv.\n",
      "Too many NaN for data in 600958.SS.csv.\n",
      "2765 rows droped from 600989.SS.csv.\n",
      "Too many NaN for data in 600989.SS.csv.\n",
      "471 rows droped from 600999.SS.csv.\n",
      "Too many NaN for data in 600999.SS.csv.\n",
      "21 rows droped from 601006.SS.csv.\n",
      "15 rows droped from 601009.SS.csv.\n",
      "1075 rows droped from 601012.SS.csv.\n",
      "Too many NaN for data in 601012.SS.csv.\n",
      "1737 rows droped from 601021.SS.csv.\n",
      "Too many NaN for data in 601021.SS.csv.\n",
      "2553 rows droped from 601066.SS.csv.\n",
      "Too many NaN for data in 601066.SS.csv.\n",
      "70 rows droped from 601088.SS.csv.\n",
      "969 rows droped from 601100.SS.csv.\n",
      "Too many NaN for data in 601100.SS.csv.\n",
      "48 rows droped from 601111.SS.csv.\n",
      "501 rows droped from 601117.SS.csv.\n",
      "Too many NaN for data in 601117.SS.csv.\n",
      "2543 rows droped from 601138.SS.csv.\n",
      "Too many NaN for data in 601138.SS.csv.\n",
      "1935 rows droped from 601155.SS.csv.\n",
      "Too many NaN for data in 601155.SS.csv.\n",
      "22 rows droped from 601166.SS.csv.\n",
      "47 rows droped from 601169.SS.csv.\n",
      "59 rows droped from 601186.SS.csv.\n",
      "1820 rows droped from 601211.SS.csv.\n",
      "Too many NaN for data in 601211.SS.csv.\n",
      "880 rows droped from 601216.SS.csv.\n",
      "Too many NaN for data in 601216.SS.csv.\n",
      "1477 rows droped from 601225.SS.csv.\n",
      "Too many NaN for data in 601225.SS.csv.\n",
      "2159 rows droped from 601229.SS.csv.\n",
      "Too many NaN for data in 601229.SS.csv.\n",
      "2803 rows droped from 601236.SS.csv.\n",
      "Too many NaN for data in 601236.SS.csv.\n",
      "1042 rows droped from 601238.SS.csv.\n",
      "Too many NaN for data in 601238.SS.csv.\n",
      "623 rows droped from 601288.SS.csv.\n",
      "Too many NaN for data in 601288.SS.csv.\n",
      "58 rows droped from 601318.SS.csv.\n",
      "2654 rows droped from 601319.SS.csv.\n",
      "Too many NaN for data in 601319.SS.csv.\n",
      "9 rows droped from 601328.SS.csv.\n",
      "991 rows droped from 601336.SS.csv.\n",
      "Too many NaN for data in 601336.SS.csv.\n",
      "1110 rows droped from 601360.SS.csv.\n",
      "Too many NaN for data in 601360.SS.csv.\n",
      "695 rows droped from 601377.SS.csv.\n",
      "Too many NaN for data in 601377.SS.csv.\n",
      "92 rows droped from 601390.SS.csv.\n",
      "16 rows droped from 601398.SS.csv.\n",
      "121 rows droped from 601600.SS.csv.\n",
      "10 rows droped from 601601.SS.csv.\n",
      "2700 rows droped from 601615.SS.csv.\n",
      "Too many NaN for data in 601615.SS.csv.\n",
      "501 rows droped from 601618.SS.csv.\n",
      "Too many NaN for data in 601618.SS.csv.\n",
      "8 rows droped from 601628.SS.csv.\n",
      "934 rows droped from 601633.SS.csv.\n",
      "Too many NaN for data in 601633.SS.csv.\n",
      "2905 rows droped from 601658.SS.csv.\n",
      "Too many NaN for data in 601658.SS.csv.\n",
      "386 rows droped from 601668.SS.csv.\n",
      "Too many NaN for data in 601668.SS.csv.\n",
      "1034 rows droped from 601669.SS.csv.\n",
      "Too many NaN for data in 601669.SS.csv.\n",
      "537 rows droped from 601688.SS.csv.\n",
      "Too many NaN for data in 601688.SS.csv.\n",
      "1761 rows droped from 601689.SS.csv.\n",
      "Too many NaN for data in 601689.SS.csv.\n",
      "2795 rows droped from 601698.SS.csv.\n",
      "Too many NaN for data in 601698.SS.csv.\n",
      "3318 rows droped from 601728.SS.csv.\n",
      "Too many NaN for data in 601728.SS.csv.\n",
      "244 rows droped from 601766.SS.csv.\n",
      "410 rows droped from 601788.SS.csv.\n",
      "Too many NaN for data in 601788.SS.csv.\n",
      "805 rows droped from 601799.SS.csv.\n",
      "Too many NaN for data in 601799.SS.csv.\n",
      "1020 rows droped from 601800.SS.csv.\n",
      "Too many NaN for data in 601800.SS.csv.\n",
      "11 rows droped from 601808.SS.csv.\n",
      "2931 rows droped from 601816.SS.csv.\n",
      "Too many NaN for data in 601816.SS.csv.\n",
      "658 rows droped from 601818.SS.csv.\n",
      "Too many NaN for data in 601818.SS.csv.\n",
      "2457 rows droped from 601838.SS.csv.\n",
      "Too many NaN for data in 601838.SS.csv.\n",
      "8 rows droped from 601857.SS.csv.\n",
      "2725 rows droped from 601865.SS.csv.\n",
      "Too many NaN for data in 601865.SS.csv.\n",
      "3343 rows droped from 601868.SS.csv.\n",
      "Too many NaN for data in 601868.SS.csv.\n",
      "641 rows droped from 601877.SS.csv.\n",
      "Too many NaN for data in 601877.SS.csv.\n",
      "2307 rows droped from 601878.SS.csv.\n",
      "Too many NaN for data in 601878.SS.csv.\n",
      "2206 rows droped from 601881.SS.csv.\n",
      "Too many NaN for data in 601881.SS.csv.\n",
      "499 rows droped from 601888.SS.csv.\n",
      "Too many NaN for data in 601888.SS.csv.\n",
      "30 rows droped from 601898.SS.csv.\n",
      "116 rows droped from 601899.SS.csv.\n",
      "1001 rows droped from 601901.SS.csv.\n",
      "Too many NaN for data in 601901.SS.csv.\n",
      "147 rows droped from 601919.SS.csv.\n",
      "10 rows droped from 601939.SS.csv.\n",
      "2087 rows droped from 601966.SS.csv.\n",
      "Too many NaN for data in 601966.SS.csv.\n",
      "1809 rows droped from 601985.SS.csv.\n",
      "Too many NaN for data in 601985.SS.csv.\n",
      "11 rows droped from 601988.SS.csv.\n",
      "755 rows droped from 601989.SS.csv.\n",
      "Too many NaN for data in 601989.SS.csv.\n",
      "3120 rows droped from 601995.SS.csv.\n",
      "Too many NaN for data in 601995.SS.csv.\n",
      "17 rows droped from 601998.SS.csv.\n",
      "1679 rows droped from 603019.SS.csv.\n",
      "Too many NaN for data in 603019.SS.csv.\n",
      "2679 rows droped from 603185.SS.csv.\n",
      "Too many NaN for data in 603185.SS.csv.\n",
      "2940 rows droped from 603195.SS.csv.\n",
      "Too many NaN for data in 603195.SS.csv.\n",
      "2517 rows droped from 603259.SS.csv.\n",
      "Too many NaN for data in 603259.SS.csv.\n",
      "2391 rows droped from 603260.SS.csv.\n",
      "Too many NaN for data in 603260.SS.csv.\n",
      "1482 rows droped from 603288.SS.csv.\n",
      "Too many NaN for data in 603288.SS.csv.\n",
      "2938 rows droped from 603290.SS.csv.\n",
      "Too many NaN for data in 603290.SS.csv.\n",
      "1582 rows droped from 603369.SS.csv.\n",
      "Too many NaN for data in 603369.SS.csv.\n",
      "2998 rows droped from 603392.SS.csv.\n",
      "Too many NaN for data in 603392.SS.csv.\n",
      "2531 rows droped from 603486.SS.csv.\n",
      "Too many NaN for data in 603486.SS.csv.\n",
      "2438 rows droped from 603501.SS.csv.\n",
      "Too many NaN for data in 603501.SS.csv.\n",
      "2453 rows droped from 603659.SS.csv.\n",
      "Too many NaN for data in 603659.SS.csv.\n",
      "1751 rows droped from 603799.SS.csv.\n",
      "Too many NaN for data in 603799.SS.csv.\n",
      "1659 rows droped from 603806.SS.csv.\n",
      "Too many NaN for data in 603806.SS.csv.\n",
      "2253 rows droped from 603833.SS.csv.\n",
      "Too many NaN for data in 603833.SS.csv.\n",
      "2360 rows droped from 603882.SS.csv.\n",
      "Too many NaN for data in 603882.SS.csv.\n",
      "1720 rows droped from 603899.SS.csv.\n",
      "Too many NaN for data in 603899.SS.csv.\n",
      "2302 rows droped from 603986.SS.csv.\n",
      "Too many NaN for data in 603986.SS.csv.\n",
      "1180 rows droped from 603993.SS.csv.\n",
      "Too many NaN for data in 603993.SS.csv.\n",
      "3234 rows droped from 605117.SS.csv.\n",
      "Too many NaN for data in 605117.SS.csv.\n",
      "3258 rows droped from 605499.SS.csv.\n",
      "Too many NaN for data in 605499.SS.csv.\n",
      "2810 rows droped from 688005.SS.csv.\n",
      "Too many NaN for data in 688005.SS.csv.\n",
      "2810 rows droped from 688008.SS.csv.\n",
      "Too many NaN for data in 688008.SS.csv.\n",
      "2810 rows droped from 688012.SS.csv.\n",
      "Too many NaN for data in 688012.SS.csv.\n",
      "2859 rows droped from 688036.SS.csv.\n",
      "Too many NaN for data in 688036.SS.csv.\n",
      "3068 rows droped from 688065.SS.csv.\n",
      "Too many NaN for data in 688065.SS.csv.\n",
      "2889 rows droped from 688111.SS.csv.\n",
      "Too many NaN for data in 688111.SS.csv.\n",
      "2991 rows droped from 688126.SS.csv.\n",
      "Too many NaN for data in 688126.SS.csv.\n",
      "2951 rows droped from 688169.SS.csv.\n",
      "Too many NaN for data in 688169.SS.csv.\n",
      "3330 rows droped from 688187.SS.csv.\n",
      "Too many NaN for data in 688187.SS.csv.\n",
      "3297 rows droped from 688303.SS.csv.\n",
      "Too many NaN for data in 688303.SS.csv.\n",
      "2885 rows droped from 688363.SS.csv.\n",
      "Too many NaN for data in 688363.SS.csv.\n",
      "2955 rows droped from 688396.SS.csv.\n",
      "Too many NaN for data in 688396.SS.csv.\n",
      "3053 rows droped from 688561.SS.csv.\n",
      "Too many NaN for data in 688561.SS.csv.\n",
      "3025 rows droped from 688599.SS.csv.\n",
      "Too many NaN for data in 688599.SS.csv.\n",
      "3049 rows droped from 688981.SS.csv.\n",
      "Too many NaN for data in 688981.SS.csv.\n"
     ]
    }
   ],
   "source": [
    "NA_THRESHOLD = 0.1\n",
    "\n",
    "calendar = get_calendar_with_tushare(TRAIN_START_DAY, TRADE_END_DAY)\n",
    "\n",
    "na_list = []\n",
    "_, _, files = next(os.walk(RAW_DATA_DIR))\n",
    "for file in files:\n",
    "    result_path = os.path.join(CLEAN_DATA_DIR, file)\n",
    "    if os.path.exists(result_path):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    df = pd.read_csv(file_path, index_col=False)\n",
    "\n",
    "    df = clean_data_from_yfinance(df, calendar)\n",
    "\n",
    "    len_df = df.shape[0]\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(['open', 'high', 'low', 'close', 'volume'], inplace=True)\n",
    "    print(f'{len_df - df.shape[0]} rows droped from {file}.')\n",
    "\n",
    "    if df.shape[0] >= len(calendar) * (1 - NA_THRESHOLD):\n",
    "        df.to_csv(result_path, index=False)\n",
    "    else:\n",
    "        na_list.append(file)\n",
    "        print(f'{file}: too many NaNs, discard.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'000333.SZ.csv 000408.SZ.csv 000776.SZ.csv 000792.SZ.csv 001289.SZ.csv 002129.SZ.csv 002252.SZ.csv 002304.SZ.csv 002311.SZ.csv 002352.SZ.csv 002371.SZ.csv 002410.SZ.csv 002414.SZ.csv 002415.SZ.csv 002459.SZ.csv 002460.SZ.csv 002466.SZ.csv 002475.SZ.csv 002493.SZ.csv 002555.SZ.csv 002594.SZ.csv 002600.SZ.csv 002601.SZ.csv 002602.SZ.csv 002648.SZ.csv 002709.SZ.csv 002714.SZ.csv 002736.SZ.csv 002756.SZ.csv 002812.SZ.csv 002821.SZ.csv 002841.SZ.csv 002916.SZ.csv 002920.SZ.csv 002938.SZ.csv 003816.SZ.csv 300014.SZ.csv 300015.SZ.csv 300033.SZ.csv 300059.SZ.csv 300122.SZ.csv 300124.SZ.csv 300142.SZ.csv 300207.SZ.csv 300223.SZ.csv 300274.SZ.csv 300316.SZ.csv 300347.SZ.csv 300408.SZ.csv 300413.SZ.csv 300433.SZ.csv 300450.SZ.csv 300454.SZ.csv 300496.SZ.csv 300498.SZ.csv 300529.SZ.csv 300595.SZ.csv 300601.SZ.csv 300628.SZ.csv 300661.SZ.csv 300750.SZ.csv 300751.SZ.csv 300759.SZ.csv 300760.SZ.csv 300763.SZ.csv 300769.SZ.csv 300782.SZ.csv 300896.SZ.csv 300919.SZ.csv 300957.SZ.csv 300979.SZ.csv 300999.SZ.csv 600025.SS.csv 600745.SS.csv 600893.SS.csv 600900.SS.csv 600905.SS.csv 600918.SS.csv 600919.SS.csv 600926.SS.csv 600941.SS.csv 600958.SS.csv 600989.SS.csv 600999.SS.csv 601012.SS.csv 601021.SS.csv 601066.SS.csv 601100.SS.csv 601117.SS.csv 601138.SS.csv 601155.SS.csv 601211.SS.csv 601216.SS.csv 601225.SS.csv 601229.SS.csv 601236.SS.csv 601238.SS.csv 601288.SS.csv 601319.SS.csv 601336.SS.csv 601360.SS.csv 601377.SS.csv 601615.SS.csv 601618.SS.csv 601633.SS.csv 601658.SS.csv 601668.SS.csv 601669.SS.csv 601688.SS.csv 601689.SS.csv 601698.SS.csv 601728.SS.csv 601788.SS.csv 601799.SS.csv 601800.SS.csv 601816.SS.csv 601818.SS.csv 601838.SS.csv 601865.SS.csv 601868.SS.csv 601877.SS.csv 601878.SS.csv 601881.SS.csv 601888.SS.csv 601901.SS.csv 601966.SS.csv 601985.SS.csv 601989.SS.csv 601995.SS.csv 603019.SS.csv 603185.SS.csv 603195.SS.csv 603259.SS.csv 603260.SS.csv 603288.SS.csv 603290.SS.csv 603369.SS.csv 603392.SS.csv 603486.SS.csv 603501.SS.csv 603659.SS.csv 603799.SS.csv 603806.SS.csv 603833.SS.csv 603882.SS.csv 603899.SS.csv 603986.SS.csv 603993.SS.csv 605117.SS.csv 605499.SS.csv 688005.SS.csv 688008.SS.csv 688012.SS.csv 688036.SS.csv 688065.SS.csv 688111.SS.csv 688126.SS.csv 688169.SS.csv 688187.SS.csv 688303.SS.csv 688363.SS.csv 688396.SS.csv 688561.SS.csv 688599.SS.csv 688981.SS.csv'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(na_list))\n",
    "' '.join(na_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'volume', 'change', 'rs_14', 'rsi',\n",
       "       'rsi_14', 'stochrsi', 'rate', 'middle', 'tp', 'boll', 'boll_ub',\n",
       "       'boll_lb', 'macd', 'macds', 'macdh', 'ppo', 'ppos', 'ppoh', 'rsv_9',\n",
       "       'kdjk_9', 'kdjk', 'kdjd_9', 'kdjd', 'kdjj_9', 'kdjj', 'cr', 'cr-ma1',\n",
       "       'cr-ma2', 'cr-ma3', 'cci', 'tr', 'atr', 'high_delta', 'um', 'low_delta',\n",
       "       'dm', 'pdm', 'pdm_14_ema', 'pdm_14', 'atr_14', 'pdi_14', 'pdi', 'mdm',\n",
       "       'mdm_14_ema', 'mdm_14', 'mdi_14', 'mdi', 'dx_14', 'dx', 'adx', 'adxr',\n",
       "       'trix', 'tema', 'vr', 'close_10_sma', 'close_50_sma', 'dma', 'vwma',\n",
       "       'chop', 'log-ret', 'mfi', 'wt1', 'wt2', 'wr', 'supertrend_ub',\n",
       "       'supertrend_lb', 'supertrend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns after init_all()\n",
    "df = pd.read_csv('./datasets/clean/000001.SZ.csv', index_col=False)\n",
    "stats = stockstats.StockDataFrame.retype(df)\n",
    "stats.init_all()\n",
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 000001.SZ.csv.\n",
      "Added 000002.SZ.csv.\n",
      "Added 000063.SZ.csv.\n",
      "Added 000069.SZ.csv.\n",
      "Added 000100.SZ.csv.\n",
      "Added 000157.SZ.csv.\n",
      "Added 000166.SZ.csv.\n",
      "Added 000301.SZ.csv.\n",
      "Added 000338.SZ.csv.\n",
      "Added 000425.SZ.csv.\n",
      "Added 000538.SZ.csv.\n",
      "Added 000568.SZ.csv.\n",
      "Added 000596.SZ.csv.\n",
      "Added 000625.SZ.csv.\n",
      "Added 000651.SZ.csv.\n",
      "Added 000661.SZ.csv.\n",
      "Added 000708.SZ.csv.\n",
      "Added 000723.SZ.csv.\n",
      "Added 000725.SZ.csv.\n",
      "Added 000733.SZ.csv.\n",
      "Added 000768.SZ.csv.\n",
      "Added 000786.SZ.csv.\n",
      "Added 000800.SZ.csv.\n",
      "Added 000858.SZ.csv.\n",
      "Added 000876.SZ.csv.\n",
      "Added 000877.SZ.csv.\n",
      "Added 000895.SZ.csv.\n",
      "Added 000938.SZ.csv.\n",
      "Added 000963.SZ.csv.\n",
      "Added 000977.SZ.csv.\n",
      "Added 001979.SZ.csv.\n",
      "Added 002001.SZ.csv.\n",
      "Added 002007.SZ.csv.\n",
      "Added 002008.SZ.csv.\n",
      "Added 002027.SZ.csv.\n",
      "Added 002032.SZ.csv.\n",
      "Added 002049.SZ.csv.\n",
      "Added 002050.SZ.csv.\n",
      "Added 002064.SZ.csv.\n",
      "Added 002074.SZ.csv.\n",
      "Added 002120.SZ.csv.\n",
      "Added 002142.SZ.csv.\n",
      "Added 002179.SZ.csv.\n",
      "Added 002180.SZ.csv.\n",
      "Added 002202.SZ.csv.\n",
      "Added 002230.SZ.csv.\n",
      "Added 002236.SZ.csv.\n",
      "Added 002241.SZ.csv.\n",
      "Added 002271.SZ.csv.\n",
      "Added 600000.SS.csv.\n",
      "Added 600009.SS.csv.\n",
      "Added 600010.SS.csv.\n",
      "Added 600011.SS.csv.\n",
      "Added 600015.SS.csv.\n",
      "Added 600016.SS.csv.\n",
      "Added 600018.SS.csv.\n",
      "Added 600019.SS.csv.\n",
      "Added 600028.SS.csv.\n",
      "Added 600029.SS.csv.\n",
      "Added 600030.SS.csv.\n",
      "Added 600031.SS.csv.\n",
      "Added 600036.SS.csv.\n",
      "Added 600039.SS.csv.\n",
      "Added 600048.SS.csv.\n",
      "Added 600050.SS.csv.\n",
      "Added 600061.SS.csv.\n",
      "Added 600085.SS.csv.\n",
      "Added 600089.SS.csv.\n",
      "Added 600104.SS.csv.\n",
      "Added 600111.SS.csv.\n",
      "Added 600115.SS.csv.\n",
      "Added 600132.SS.csv.\n",
      "Added 600150.SS.csv.\n",
      "Added 600176.SS.csv.\n",
      "Added 600183.SS.csv.\n",
      "Added 600188.SS.csv.\n",
      "Added 600196.SS.csv.\n",
      "Added 600219.SS.csv.\n",
      "Added 600233.SS.csv.\n",
      "Added 600276.SS.csv.\n",
      "Added 600309.SS.csv.\n",
      "Added 600332.SS.csv.\n",
      "Added 600346.SS.csv.\n",
      "Added 600362.SS.csv.\n",
      "Added 600383.SS.csv.\n",
      "Added 600406.SS.csv.\n",
      "Added 600426.SS.csv.\n",
      "Added 600436.SS.csv.\n",
      "Added 600438.SS.csv.\n",
      "Added 600460.SS.csv.\n",
      "Added 600519.SS.csv.\n",
      "Added 600547.SS.csv.\n",
      "Added 600570.SS.csv.\n",
      "Added 600584.SS.csv.\n",
      "Added 600585.SS.csv.\n",
      "Added 600588.SS.csv.\n",
      "Added 600600.SS.csv.\n",
      "Added 600606.SS.csv.\n",
      "Added 600660.SS.csv.\n",
      "Added 600674.SS.csv.\n",
      "Added 600690.SS.csv.\n",
      "Added 600741.SS.csv.\n",
      "Added 600760.SS.csv.\n",
      "Added 600763.SS.csv.\n",
      "Added 600795.SS.csv.\n",
      "Added 600803.SS.csv.\n",
      "Added 600809.SS.csv.\n",
      "Added 600837.SS.csv.\n",
      "Added 600845.SS.csv.\n",
      "Added 600884.SS.csv.\n",
      "Added 600886.SS.csv.\n",
      "Added 600887.SS.csv.\n",
      "Added 601006.SS.csv.\n",
      "Added 601009.SS.csv.\n",
      "Added 601088.SS.csv.\n",
      "Added 601111.SS.csv.\n",
      "Added 601166.SS.csv.\n",
      "Added 601169.SS.csv.\n",
      "Added 601186.SS.csv.\n",
      "Added 601318.SS.csv.\n",
      "Added 601328.SS.csv.\n",
      "Added 601390.SS.csv.\n",
      "Added 601398.SS.csv.\n",
      "Added 601600.SS.csv.\n",
      "Added 601601.SS.csv.\n",
      "Added 601628.SS.csv.\n",
      "Added 601766.SS.csv.\n",
      "Added 601808.SS.csv.\n",
      "Added 601857.SS.csv.\n",
      "Added 601898.SS.csv.\n",
      "Added 601899.SS.csv.\n",
      "Added 601919.SS.csv.\n",
      "Added 601939.SS.csv.\n",
      "Added 601988.SS.csv.\n",
      "Added 601998.SS.csv.\n"
     ]
    }
   ],
   "source": [
    "X_y_filename = 'x_y.csv'\n",
    "X_y_path = os.path.join(DATA_SAVE_DIR, X_y_filename)\n",
    "\n",
    "if not os.path.exists(X_y_path):\n",
    "    X_y = None\n",
    "\n",
    "    _, _, files = next(os.walk(CLEAN_DATA_DIR))\n",
    "    for file in files:\n",
    "        file_path = os.path.join(CLEAN_DATA_DIR, file)\n",
    "        df = pd.read_csv(file_path, index_col=False)\n",
    "        stats = stockstats.StockDataFrame.retype(df)\n",
    "        stats.init_all()\n",
    "\n",
    "        # drop duplicated columns\n",
    "        stats.drop_column(['rsi', 'kdjk', 'kdjd', 'kdjj'], inplace=True)\n",
    "\n",
    "        # add additional indicators: close_14_smma, close_14_mstd, close_14_mvar,\n",
    "        # close_5_sma, wr_6, rsi_6,\n",
    "        # log differential of high, low, open and volume\n",
    "        # and log2(close / open)\n",
    "        stats['close_14_smma']; stats['close_14_mstd']; stats['close_14_mvar'];\n",
    "        stats['close_5_sma'];   stats['wr_6'];  stats['rsi_6']\n",
    "        stats['log_diff_high'] = np.log2(stats['high'] / stats['high_-1_s'])\n",
    "        stats['log_diff_low']= np.log2(stats['low'] / stats['low_-1_s'])\n",
    "        stats['log_diff_open']= np.log2(stats['open'] / stats['open_-1_s'])\n",
    "        stats['log_diff_vol']= np.log2(stats['volume'] / stats['volume_-1_s'])\n",
    "        stats['log_close/open'] = np.log2(stats['close'] / stats['open'])\n",
    "        stats.drop_column(['high_-1_s', 'low_-1_s', 'open_-1_s'], inplace=True)\n",
    "        stats['log-ret_1_s']\n",
    "        stats.rename(columns={'log-ret_1_s': 'y'}, inplace=True)\n",
    "\n",
    "        # drop date\n",
    "        stats.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # deal with nan\n",
    "        stats.dropna(inplace=True)\n",
    "\n",
    "        if X_y is None:\n",
    "            X_y = stats.copy()\n",
    "        else:\n",
    "            X_y = pd.concat([X_y, stats])\n",
    "        print(f'Add {file} to X_y.')\n",
    "\n",
    "X_y.to_csv(X_y_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_filename = 'x_y.csv'\n",
    "X_y_path = os.path.join(DATA_SAVE_DIR, X_y_filename)\n",
    "if X_y is None:\n",
    "    X_y = pd.read_csv(X_y_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "feat_selector = BorutaPy(\n",
    "    verbose=2,\n",
    "    estimator=model,\n",
    "    n_estimators='auto',\n",
    "    max_iter=10\n",
    ")\n",
    "\n",
    "X = np.array(X_y.drop(labels=['y']))\n",
    "y = np.array(X_y['y'])\n",
    "feat_selector.fit(X, y)\n",
    "\n",
    "# print support and ranking for each feature\n",
    "print(\"\\n------Support and Ranking for each feature------\")\n",
    "for i in range(len(feat_selector.support_)):\n",
    "    if feat_selector.support_[i]:\n",
    "        print(\"Passes the test: \", X.columns[i],\n",
    "              \" - Ranking: \", feat_selector.ranking_[i])\n",
    "    else:\n",
    "        print(\"Doesn't pass the test: \",\n",
    "              X.columns[i], \" - Ranking: \", feat_selector.ranking_[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "* volume\n",
    "* ppo\n",
    "* cr-ma3\n",
    "* trix \n",
    "* log_diff_high\n",
    "* log_diff_low\n",
    "* log_diff_open\n",
    "* log_close/open"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "\n",
    "_, _, files = next(os.walk(CLEAN_DATA_DIR))\n",
    "for file in files:\n",
    "    # skip if already exists\n",
    "    processed_file_path = os.path.join(PREPROCESSED_DATA_DIR, file)\n",
    "    if os.path.exists(processed_file_path):\n",
    "        continue\n",
    "    \n",
    "    # load\n",
    "    clean_file_path = os.path.join(CLEAN_DATA_DIR, file)\n",
    "    stats = pd.read_csv(clean_file_path, index_col=False)\n",
    "    df = pd.DataFrame(index=stats['date'])\n",
    "\n",
    "    stats = stockstats.StockDataFrame.retype(stats)\n",
    "    df['close'] = stats['close']\n",
    "\n",
    "    # add indicators\n",
    "    df['ppo'] = stats['ppo']\n",
    "    df['cr-ma3'] = stats['cr-ma3']\n",
    "    df['trix'] = stats['trix']\n",
    "\n",
    "    # add differential features\n",
    "    df['log_close/open'] = np.log2(stats['close'] / stats['open'])\n",
    "    df['log-ret'] = stats['log-ret']\n",
    "    df['log_diff_high'] = np.log2(stats['high'] / stats['high_-1_s'])\n",
    "    df['log_diff_low'] = np.log2(stats['low'] / stats['low_-1_s'])\n",
    "    df['log_diff_open'] = np.log2(stats['open'] / stats['open_-1_s'])\n",
    "\n",
    "    # clean\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # save\n",
    "    df.to_csv(processed_file_path, index=False)\n",
    "    tic = file.split('.')[0]\n",
    "    df_dict[tic] = df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_factory(dfs: List[pd.DataFrame]) -> SingleStockTradingEnv:\n",
    "    '''wrap single stock trading environment with monitor.'''\n",
    "    return Monitor(SingleStockTradingEnv(dfs, 5000_000, stack_frame=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just load data\n",
    "df_dict = {}\n",
    "_, _, files = next(os.walk(PREPROCESSED_DATA_DIR))\n",
    "for file in files:\n",
    "    processed_file_path = os.path.join(PREPROCESSED_DATA_DIR, file)   \n",
    "    df = pd.read_csv(processed_file_path, index_col=False)\n",
    "    assert df.isna().sum().sum() == 0, f'Nan found in {file}.'\n",
    "    tic = file.split('.')[0]\n",
    "    df_dict[tic] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "df_dict_train = dict()\n",
    "df_dict_test = dict()\n",
    "df_dict_trade = dict()\n",
    "\n",
    "TEST_START_DAY = pd.to_datetime(TEST_START_DAY, format='%Y-%m-%d')\n",
    "TRADE_START_DAY = pd.to_datetime(TRADE_START_DAY, format='%Y-%m-%d')\n",
    "\n",
    "for tic, df in df_dict.items():\n",
    "    df.date = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "    df_dict_train[tic] = df.loc[df.date < TEST_START_DAY].sort_index(ascending=True).copy()\n",
    "    df_dict_test[tic] = df.loc[(df.date >= TEST_START_DAY) & (df.date < TRADE_START_DAY)].sort_index(ascending=True).copy()\n",
    "    df_dict_trade[tic] = df.loc[df.date >= TRADE_START_DAY].sort_index(ascending=True).copy()\n",
    "\n",
    "# create env\n",
    "env_train = env_factory(list(df_dict_train.values()))\n",
    "env_test = env_factory(list(df_dict_test.values()))\n",
    "env_trade = env_factory(list(df_dict_trade.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(\n",
    "    model_name: str, \n",
    "    model_class: Type[BaseAlgorithm], \n",
    "    sample_param_func: Callable[[optuna.Trial], Tuple[Dict, int]],\n",
    "    ) -> Callable[[optuna.Trial], float]:\n",
    "    \n",
    "    def objective(trial: optuna.Trial):\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "        model_path = os.path.join(model_path, f'trial_{trial.number}_best_model')\n",
    "        tb_log_path = os.path.join(TENSORBOARD_LOG_DIR, model_name)\n",
    "        check_and_make_directories([model_path, tb_log_path])\n",
    "\n",
    "        # Create model with sampled hyperparameters and \n",
    "        # train it with early stop callback    \n",
    "        hyperparameters, total_timesteps = sample_param_func(trial)\n",
    "        hyperparameters['tensorboard_log'] = '/root/tf-logs/' # tb_log_path #\n",
    "\n",
    "        model = model_class('MlpPolicy', env_train, **hyperparameters)\n",
    "\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(\n",
    "            max_no_improvement_evals=4, min_evals=2, verbose=VERBOSE)\n",
    "        eval_callback = MaskableEvalCallback(\n",
    "            env_test, \n",
    "            callback_after_eval=stop_train_callback,\n",
    "            n_eval_episodes=3,\n",
    "            eval_freq=10000,\n",
    "            best_model_save_path=model_path, \n",
    "            verbose=VERBOSE\n",
    "            )\n",
    "        try:\n",
    "            model.learn(total_timesteps=total_timesteps, \n",
    "                tb_log_name=f'{model_name}_{trial.number}', callback=eval_callback)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return -999\n",
    "\n",
    "        # validation\n",
    "        mean_reward, _ = evaluate_policy(model, env_test, n_eval_episodes=3)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(\n",
    "    model_name: str, \n",
    "    model_class: Type[BaseAlgorithm],\n",
    "    sample_param_func: Callable[[optuna.Trial], Any],\n",
    "    n_trials: int = 100, \n",
    "    callbacks: List[Callable] = None\n",
    "    ) -> optuna.Study:\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=None)\n",
    "    objective = objective_factory(model_name, model_class, sample_param_func)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f'{model_name}_study', \n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "        pruner=optuna.pruners.HyperbandPruner()\n",
    "        )\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=n_trials,\n",
    "        callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with strict condition\n",
    "early_stop_callback = PruneCallback(\n",
    "    threshold=1,\n",
    "    patience=1,\n",
    "    trial_number=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-31 13:00:33,568]\u001b[0m A new study created in memory with name: MaskableDQN_study\u001b[0m\n",
      "\u001b[33m[W 2023-01-31 13:03:33,532]\u001b[0m Trial 0 failed because of the following error: TypeError(\"cannot pickle 'tensorflow.python.lib.io._pywrap_file_io.WritableFile' object\")\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_643908/1085890602.py\", line 31, in objective\n",
      "    model.learn(total_timesteps=total_timesteps,\n",
      "  File \"/root/Experiment/StockTrading/maskable/MaskableDQN.py\", line 118, in learn\n",
      "    super().learn(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py\", line 265, in learn\n",
      "    return super().learn(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 334, in learn\n",
      "    rollout = self.collect_rollouts(\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py\", line 575, in collect_rollouts\n",
      "    if callback.on_step() is False:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py\", line 100, in on_step\n",
      "    return self._on_step()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/sb3_contrib/common/maskable/callbacks.py\", line 115, in _on_step\n",
      "    self.model.save(os.path.join(self.best_model_save_path, \"best_model\"))\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/base_class.py\", line 813, in save\n",
      "    save_to_zip_file(path, data=data, params=params_to_save, pytorch_variables=pytorch_variables)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py\", line 311, in save_to_zip_file\n",
      "    serialized_data = data_to_json(data)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py\", line 99, in data_to_json\n",
      "    base64_encoded = base64.b64encode(cloudpickle.dumps(data_item)).decode()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py\", line 633, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle 'tensorflow.python.lib.io._pywrap_file_io.WritableFile' object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'tensorflow.python.lib.io._pywrap_file_io.WritableFile' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# study_mppo = tune('MaskablePPO', MaskablePPO, \\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#     sample_param_func=sample_mppo_param)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m study_mdqn \u001b[39m=\u001b[39m tune(\u001b[39m'\u001b[39;49m\u001b[39mMaskableDQN\u001b[39;49m\u001b[39m'\u001b[39;49m, MaskableDQN, \\\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     sample_param_func\u001b[39m=\u001b[39;49msample_mdqn_param)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# study_mppo = tune('MaskableQRDQN', MaskableQRDQN, \\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     sample_param_func=sample_mqrdqn_param)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# study_mdqn = tune('MaskableIQN', MaskableIQN, \\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     sample_param_func=sample_miqn_param)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m plot_optimization_history(study_mdqn)\n",
      "\u001b[1;32m/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb Cell 36\u001b[0m in \u001b[0;36mtune\u001b[0;34m(model_name, model_class, sample_param_func, n_trials, callbacks)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m objective \u001b[39m=\u001b[39m objective_factory(model_name, model_class, sample_param_func)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_study\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     sampler\u001b[39m=\u001b[39msampler,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mHyperbandPruner()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     objective, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m study\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb Cell 36\u001b[0m in \u001b[0;36mobjective_factory.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m eval_callback \u001b[39m=\u001b[39m MaskableEvalCallback(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     env_test, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     callback_after_eval\u001b[39m=\u001b[39mstop_train_callback,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     verbose\u001b[39m=\u001b[39mVERBOSE\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mtrial\u001b[39m.\u001b[39;49mnumber\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, callback\u001b[39m=\u001b[39;49meval_callback)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bregion-41.seetacloud.com/root/Experiment/StockTrading/ExpSingleStockTrading.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/Experiment/StockTrading/maskable/MaskableDQN.py:118\u001b[0m, in \u001b[0;36mMaskableDQN.learn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\u001b[39mself\u001b[39m: SelfMaskableDQN, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfMaskableDQN:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:265\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[1;32m    257\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    266\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    267\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    268\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    269\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    270\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    271\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:334\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    331\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    333\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 334\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    335\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    336\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    337\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    338\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    339\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    340\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    341\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:575\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    573\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[1;32m    574\u001b[0m \u001b[39m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutReturn(num_collected_steps \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mnum_envs, num_collected_episodes, continue_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    578\u001b[0m \u001b[39m# Retrieve reward and episode length if using Monitor wrapper\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/callbacks.py:100\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sb3_contrib/common/maskable/callbacks.py:115\u001b[0m, in \u001b[0;36mMaskableEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNew best mean reward!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_save_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_model_save_path, \u001b[39m\"\u001b[39;49m\u001b[39mbest_model\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_mean_reward \u001b[39m=\u001b[39m mean_reward\n\u001b[1;32m    117\u001b[0m \u001b[39m# Trigger callback on new best model, if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:813\u001b[0m, in \u001b[0;36mBaseAlgorithm.save\u001b[0;34m(self, path, exclude, include)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39m# Build dict of state_dicts\u001b[39;00m\n\u001b[1;32m    811\u001b[0m params_to_save \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_parameters()\n\u001b[0;32m--> 813\u001b[0m save_to_zip_file(path, data\u001b[39m=\u001b[39;49mdata, params\u001b[39m=\u001b[39;49mparams_to_save, pytorch_variables\u001b[39m=\u001b[39;49mpytorch_variables)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:311\u001b[0m, in \u001b[0;36msave_to_zip_file\u001b[0;34m(save_path, data, params, pytorch_variables, verbose)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39m# data/params can be None, so do not\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m# try to serialize them blindly\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     serialized_data \u001b[39m=\u001b[39m data_to_json(data)\n\u001b[1;32m    313\u001b[0m \u001b[39m# Create a zip-archive and write our objects there.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39mwith\u001b[39;00m zipfile\u001b[39m.\u001b[39mZipFile(save_path, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m archive:\n\u001b[1;32m    315\u001b[0m     \u001b[39m# Do not try to save \"None\" elements\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:99\u001b[0m, in \u001b[0;36mdata_to_json\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     92\u001b[0m     serializable_data[data_key] \u001b[39m=\u001b[39m data_item\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39m# Not serializable, cloudpickle it into\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# bytes and convert to base64 string for storing.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39m# Also store type of the class for consumption\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39m# from other languages/humans, so we have an\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39m# idea what was being stored.\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     base64_encoded \u001b[39m=\u001b[39m base64\u001b[39m.\u001b[39mb64encode(cloudpickle\u001b[39m.\u001b[39;49mdumps(data_item))\u001b[39m.\u001b[39mdecode()\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Use \":\" to make sure we do\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# not override these keys\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[39m# when we include variables of the object later\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     cloudpickle_serialization \u001b[39m=\u001b[39m {\n\u001b[1;32m    105\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m:type:\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(data_item)),\n\u001b[1;32m    106\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m:serialized:\u001b[39m\u001b[39m\"\u001b[39m: base64_encoded,\n\u001b[1;32m    107\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     70\u001b[0m     cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m         file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m     )\n\u001b[0;32m---> 73\u001b[0m     cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/cloudpickle/cloudpickle_fast.py:633\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m    632\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m         \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[1;32m    634\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    635\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrecursion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'tensorflow.python.lib.io._pywrap_file_io.WritableFile' object"
     ]
    }
   ],
   "source": [
    "# study_mppo = tune('MaskablePPO', MaskablePPO, \\\n",
    "#     sample_param_func=sample_mppo_param)\n",
    "study_mdqn = tune('MaskableDQN', MaskableDQN, \\\n",
    "    sample_param_func=sample_mdqn_param)\n",
    "# study_mppo = tune('MaskableQRDQN', MaskableQRDQN, \\\n",
    "#     sample_param_func=sample_mqrdqn_param)\n",
    "# study_mdqn = tune('MaskableIQN', MaskableIQN, \\\n",
    "#     sample_param_func=sample_miqn_param)\n",
    "\n",
    "plot_optimization_history(study_mdqn)\n",
    "plot_param_importances(study_mdqn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x23c6647a220>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MaskablePPO('MlpPolicy', env_train)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_t = dfs_test[3]\n",
    "# list_asset, actions = simulate_trading_masked(env_factory([df_t]), model)\n",
    "# sr_asset = pd.Series(list_asset)\n",
    "# sr_return = get_daily_return(sr_asset)\n",
    "# backtest_stats(sr_return)\n",
    "# sr_baseline_return = get_daily_return(df_t.close).dropna()\n",
    "# sr_baseline_return = sr_baseline_return[len(sr_baseline_return) - len(sr_asset):]\n",
    "# backtest_stats(sr_baseline_return)\n",
    "# %matplotlib inline\n",
    "# sr_date = df_t.date\n",
    "# sr_date = sr_date[len(sr_date) - len(sr_asset):]\n",
    "# sr_return.set_axis(sr_date, inplace=True)\n",
    "# sr_baseline_return.set_axis(sr_date, inplace=True)\n",
    "# backtest_plot(sr_return, sr_baseline_return)\n",
    "# sum(actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for maskable PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2023-01-30 11:35:09,452] A new study created in memory with name: MaskablePPO_study\n",
    "[I 2023-01-30 11:45:22,296] Trial 0 finished with value: -1.0710616666666666 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 2, 'batch_size_2exp': 5, 'n_epochs': 3, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 0 with value: -1.0710616666666666.\n",
    "[I 2023-01-30 11:54:56,256] Trial 1 finished with value: -0.2739243333333334 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 5, 'batch_size_2exp': 7, 'n_epochs': 2, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 1 with value: -0.2739243333333334.\n",
    "[I 2023-01-30 11:59:04,286] Trial 2 finished with value: -0.7405313333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 7, 'batch_size_2exp': 8, 'n_epochs': 1, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 1 with value: -0.2739243333333334.\n",
    "[I 2023-01-30 12:07:29,519] Trial 3 finished with value: -0.3807473333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 1 with value: -0.2739243333333334.\n",
    "[I 2023-01-30 12:14:18,185] Trial 4 finished with value: -1.4296143333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 5, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 9, 'net_arch_layers': 5}. Best is trial 1 with value: -0.2739243333333334.\n",
    "[I 2023-01-30 12:27:19,143] Trial 5 finished with value: -1.0494126666666668 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 4, 'batch_size_2exp': 7, 'n_epochs': 4, 'net_arch_dim_2exp': 7, 'net_arch_layers': 4}. Best is trial 1 with value: -0.2739243333333334.\n",
    "[I 2023-01-30 12:34:46,817] Trial 6 finished with value: -0.2175826666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 6 with value: -0.2175826666666667.\n",
    "[I 2023-01-30 12:34:46,989] Trial 7 finished with value: -999.0 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 0, 'batch_size_2exp': 5, 'n_epochs': 1, 'net_arch_dim_2exp': 8, 'net_arch_layers': 4}. Best is trial 6 with value: -0.2175826666666667.\n",
    "Expected parameter logits (Tensor of shape (1, 3)) of distribution MaskableCategorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\n",
    "tensor([[nan, nan, nan]], device='cuda:0')\n",
    "[I 2023-01-30 12:40:39,196] Trial 8 finished with value: 0.139528 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 12:44:18,919] Trial 9 finished with value: -0.8220639999999998 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 7, 'batch_size_2exp': 8, 'n_epochs': 2, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 12:48:21,233] Trial 10 finished with value: 0.005568666666666666 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 12:58:03,468] Trial 11 finished with value: -0.8772513333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:05:05,425] Trial 12 finished with value: -0.5665723333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:09:48,089] Trial 13 finished with value: -0.25151299999999993 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 6, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:18:46,623] Trial 14 finished with value: -1.2781556666666667 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 8, 'batch_size_2exp': 5, 'n_epochs': 3, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:25:25,358] Trial 15 finished with value: -1.2174103333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 6, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:29:13,929] Trial 16 finished with value: -0.38875166666666666 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 7, 'batch_size_2exp': 7, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:29:14,213] Trial 17 finished with value: -999.0 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 0, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "Expected parameter logits (Tensor of shape (1, 3)) of distribution MaskableCategorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\n",
    "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SubBackward0>)\n",
    "[I 2023-01-30 13:33:41,079] Trial 18 finished with value: -0.515313 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 6, 'batch_size_2exp': 6, 'n_epochs': 3, 'net_arch_dim_2exp': 6, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:37:33,111] Trial 19 finished with value: -0.7629226666666667 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 8, 'batch_size_2exp': 7, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 13:41:35,566] Trial 20 finished with value: -0.6497763333333334 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 5, 'batch_size_2exp': 5, 'n_epochs': 2, 'net_arch_dim_2exp': 7, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 14:00:16,185] Trial 21 finished with value: -0.3992253333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 14:08:04,275] Trial 22 finished with value: -1.0690483333333332 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 4, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 14:25:59,409] Trial 23 finished with value: -0.030084666666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 14:45:40,132] Trial 24 finished with value: -1.3744766666666666 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 1, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 15:23:03,830] Trial 25 finished with value: -1.7653846666666666 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 1, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 15:28:30,068] Trial 26 finished with value: -0.7855256666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 7, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 15:38:54,512] Trial 27 finished with value: -0.6224313333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 15:49:41,475] Trial 28 finished with value: -0.3991933333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 8, 'n_epochs': 4, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 16:20:58,663] Trial 29 finished with value: -1.0415336666666666 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 1, 'batch_size_2exp': 5, 'n_epochs': 3, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 16:30:02,294] Trial 30 finished with value: -1.4698586666666669 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 4, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 16:43:21,658] Trial 31 finished with value: 0.02088833333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:00:20,585] Trial 32 finished with value: -0.8848716666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:15:36,529] Trial 33 finished with value: -0.019853 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:22:31,901] Trial 34 finished with value: -0.44696099999999994 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 8, 'n_epochs': 4, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:31:47,434] Trial 35 finished with value: -0.17429333333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 5, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:37:08,776] Trial 36 finished with value: -0.413024 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 4, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:45:36,550] Trial 37 finished with value: -0.5693386666666667 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 8, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:52:27,447] Trial 38 finished with value: -0.32163899999999995 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 6, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 17:58:10,626] Trial 39 finished with value: -0.39240733333333333 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 7, 'batch_size_2exp': 8, 'n_epochs': 1, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 18:02:35,272] Trial 40 finished with value: -0.5711713333333334 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 5, 'batch_size_2exp': 7, 'n_epochs': 3, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 18:12:29,331] Trial 41 finished with value: -0.09938799999999999 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 18:30:15,883] Trial 42 finished with value: -0.28991866666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 18:38:42,726] Trial 43 finished with value: -0.4875106666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 18:49:52,209] Trial 44 finished with value: -1.0828693333333332 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 4, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 19:35:10,493] Trial 45 finished with value: -0.6904150000000001 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 1, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 19:45:15,404] Trial 46 finished with value: -0.8048216666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 8, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 3}. Best is trial 8 with value: 0.139528.\n",
    "[I 2023-01-30 19:49:33,910] Trial 47 finished with value: 0.20815533333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 19:57:19,009] Trial 48 finished with value: -0.47554466666666667 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 4}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:04:59,025] Trial 49 finished with value: -0.7375536666666668 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:11:10,333] Trial 50 finished with value: -0.9862456666666667 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 7, 'batch_size_2exp': 5, 'n_epochs': 4, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:19:04,843] Trial 51 finished with value: -0.14732266666666669 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:26:01,710] Trial 52 finished with value: -0.7422023333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 7, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 8, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:30:08,180] Trial 53 finished with value: -0.5128739999999999 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:37:32,129] Trial 54 finished with value: -0.9192929999999998 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:52:52,159] Trial 55 finished with value: -0.131528 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 8, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 20:59:14,149] Trial 56 finished with value: -1.0313139999999998 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 7, 'batch_size_2exp': 6, 'n_epochs': 2, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 21:10:03,209] Trial 57 finished with value: -1.6049653333333334 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 4, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 21:17:36,516] Trial 58 finished with value: -0.27793833333333334 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 6, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 21:17:36,563] Trial 59 finished with value: -999.0 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 0, 'batch_size_2exp': 7, 'n_epochs': 4, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "Expected parameter logits (Tensor of shape (1, 3)) of distribution MaskableCategorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\n",
    "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SubBackward0>)\n",
    "[I 2023-01-30 21:46:16,107] Trial 60 finished with value: -0.49509800000000004 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 21:56:03,564] Trial 61 finished with value: -0.6322530000000001 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 22:12:22,781] Trial 62 finished with value: -0.7224483333333332 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 22:30:54,019] Trial 63 finished with value: 0.04822266666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 22:42:31,342] Trial 64 finished with value: 0.003159666666666665 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 22:47:32,895] Trial 65 finished with value: -0.40266699999999994 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 8, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 22:53:58,820] Trial 66 finished with value: -0.477679 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 4, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 23:32:58,481] Trial 67 finished with value: -1.2847836666666668 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 1, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 23:42:40,603] Trial 68 finished with value: -0.48326399999999997 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 2, 'batch_size_2exp': 5, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 23:54:21,760] Trial 69 finished with value: -0.39726733333333336 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 4, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-30 23:59:47,350] Trial 70 finished with value: -0.4848543333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 4, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 00:16:09,006] Trial 71 finished with value: -0.9602633333333334 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 00:35:12,061] Trial 72 finished with value: -0.3500816666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 00:43:32,720] Trial 73 finished with value: 0.007844333333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 00:52:07,215] Trial 74 finished with value: -0.342135 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 00:56:10,199] Trial 75 finished with value: -0.7481516666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 7, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:03:47,370] Trial 76 finished with value: -0.5838169999999999 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 8, 'n_epochs': 3, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:13:26,577] Trial 77 finished with value: -0.4263326666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 3, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 9, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:19:16,797] Trial 78 finished with value: -0.5381776666666668 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 5, 'batch_size_2exp': 6, 'n_epochs': 4, 'net_arch_dim_2exp': 10, 'net_arch_layers': 4}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:23:02,532] Trial 79 finished with value: 0.19475166666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 8, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:26:53,095] Trial 80 finished with value: -0.8212086666666667 and parameters: {'learning_rate_3_exp': -4, 'n_steps_2exp': 8, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:31:07,702] Trial 81 finished with value: -0.6204623333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 8, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:34:51,587] Trial 82 finished with value: -0.5214996666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 8, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:40:38,826] Trial 83 finished with value: -1.095369 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 8, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:44:33,952] Trial 84 finished with value: -0.8564336666666666 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 7, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 01:56:47,320] Trial 85 finished with value: -0.3596026666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 4, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 02:02:02,910] Trial 86 finished with value: -0.9681890000000001 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 8, 'batch_size_2exp': 6, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 02:08:41,924] Trial 87 finished with value: -0.26260633333333333 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 3, 'batch_size_2exp': 7, 'n_epochs': 4, 'net_arch_dim_2exp': 8, 'net_arch_layers': 3}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 02:12:41,794] Trial 88 finished with value: -0.786614 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 7, 'batch_size_2exp': 7, 'n_epochs': 5, 'net_arch_dim_2exp': 7, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 02:23:07,728] Trial 89 finished with value: -0.8378396666666666 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 4, 'batch_size_2exp': 8, 'n_epochs': 5, 'net_arch_dim_2exp': 6, 'net_arch_layers': 4}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 02:48:17,021] Trial 90 finished with value: 0.007555333333333334 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 1, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 02:48:17,695] Trial 91 finished with value: -999.0 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 0, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "Expected parameter logits (Tensor of shape (1, 3)) of distribution MaskableCategorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\n",
    "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SubBackward0>)\n",
    "[I 2023-01-31 03:04:46,258] Trial 92 finished with value: -0.009927333333333335 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 03:27:55,914] Trial 93 finished with value: -0.1371403333333333 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 1, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 04:02:10,640] Trial 94 finished with value: -0.0028663333333333336 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 1, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 04:47:43,578] Trial 95 finished with value: -0.26596166666666665 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 1, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 05:30:27,088] Trial 96 finished with value: -0.004809666666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 1, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "[I 2023-01-31 05:30:27,700] Trial 97 finished with value: -999.0 and parameters: {'learning_rate_3_exp': -3, 'n_steps_2exp': 0, 'batch_size_2exp': 5, 'n_epochs': 5, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333.\n",
    "Expected parameter logits (Tensor of shape (1, 3)) of distribution MaskableCategorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\n",
    "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SubBackward0>)\n",
    "[I 2023-01-31 05:48:56,050] Trial 98 finished with value: -0.37979466666666667 and parameters: {'learning_rate_3_exp': -5, 'n_steps_2exp': 2, 'batch_size_2exp': 5, 'n_epochs': 4, 'net_arch_dim_2exp': 10, 'net_arch_layers': 5}. Best is trial 47 with value: 0.20815533333333333."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "606c65d0c67f4fb4d4dcc32359b236efe9c56120e1c34efc12f6c8d1f87ab33f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
