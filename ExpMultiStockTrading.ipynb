{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/python3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/python3.9/lib/python3.9/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import typing\n",
    "from typing import Any, Callable, Dict, Type\n",
    "import warnings\n",
    "\n",
    "from boruta import BorutaPy\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_contour, plot_edf, \\\n",
    "    plot_intermediate_values, plot_optimization_history, plot_parallel_coordinate, \\\n",
    "    plot_param_importances, plot_slice\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sb3_contrib.tqc import TQC\n",
    "from stable_baselines3.a2c import A2C\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.sac import SAC\n",
    "import stockstats\n",
    "import tushare\n",
    "import yfinance as yf\n",
    "\n",
    "from environment.MultiStockTradingEnv import MultiStockTradingEnv\n",
    "from utils.sample_funcs import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Setup directories\n",
    "DATA_SAVE_DIR = 'datasets'\n",
    "MODEL_DIR = 'models'\n",
    "TENSORBOARD_LOG_DIR = 'tensorboard_log'\n",
    "RAW_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'raw')\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'clean')\n",
    "PREPROCESSED_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'preprocessed')\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, MODEL_DIR, TENSORBOARD_LOG_DIR, \\\n",
    "     RAW_DATA_DIR, CLEAN_DATA_DIR, PREPROCESSED_DATA_DIR])\n",
    "\n",
    "TRAIN_START_DAY = '2008-01-01'\n",
    "TRAIN_END_DAY = '2016-12-31'\n",
    "TEST_START_DAY = '2017-01-01'\n",
    "TEST_END_DAY = '2019-12-31'\n",
    "TRADE_START_DAY = '2020-01-01'\n",
    "TRADE_END_DAY = '2022-12-31'\n",
    "\n",
    "tushare_token = '2bf5fdb105eefda26ef27cc9caa94e6f31ca66e408f7cc54d4fce032'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve SSE 50 component list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE 50 components from http://www.sse.com.cn/market/sseindex/indexlist/basic/index.shtml?COMPANY_CODE=000016&INDEX_Code=000016&type=1\n",
    "SSE50_COM = \\\n",
    "\"\"\"包钢股份(600010)\t中国石化(600028)\t中信证券(600030)\n",
    "三一重工(600031)\t招商银行(600036)\t保利发展(600048)\n",
    "上汽集团(600104)\t北方稀土(600111)\t复星医药(600196)\n",
    "恒瑞医药(600276)\t万华化学(600309)\t恒力石化(600346)\n",
    "国电南瑞(600406)\t片仔癀(600436)\t通威股份(600438)\n",
    "贵州茅台(600519)\t海螺水泥(600585)\t海尔智家(600690)\n",
    "闻泰科技(600745)\t山西汾酒(600809)\t伊利股份(600887)\n",
    "航发动力(600893)\t长江电力(600900)\t三峡能源(600905)\n",
    "隆基绿能(601012)\t中信建投(601066)\t中国神华(601088)\n",
    "兴业银行(601166)\t陕西煤业(601225)\t农业银行(601288)\n",
    "中国平安(601318)\t工商银行(601398)\t中国太保(601601)\n",
    "中国人寿(601628)\t长城汽车(601633)\t中国建筑(601668)\n",
    "中国电建(601669)\t华泰证券(601688)\t中国石油(601857)\n",
    "中国中免(601888)\t紫金矿业(601899)\t中远海控(601919)\n",
    "中金公司(601995)\t药明康德(603259)\t合盛硅业(603260)\n",
    "海天味业(603288)\t韦尔股份(603501)\t华友钴业(603799)\n",
    "兆易创新(603986)\t天合光能(688599)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_list = re.findall(r'\\d{6}', SSE50_COM)\n",
    "tic_list = [tic+'.SS' for tic in tic_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download SSE50 tickers with yfinace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_with_yfince(tic_list: List[str], download_dir: str) -> List[str]:\n",
    "    retry_list = []\n",
    "    for tic in tic_list:\n",
    "        csv_path = os.path.join(download_dir, f'{tic}.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            print(f'File {csv_path} already exist. Skip')\n",
    "            continue\n",
    "        \n",
    "        ticker = yf.Ticker(tic)\n",
    "        df = ticker.history(period='max')\n",
    "        if df.shape[0] > 0:\n",
    "            df.to_csv(csv_path)\n",
    "            print(f'Download {tic}.csv')         \n",
    "            time.sleep(0.1)\n",
    "        else:\n",
    "            retry_list.append(tic)\n",
    "    \n",
    "    return retry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 600010.SS.csv\n",
      "Download 600028.SS.csv\n",
      "Download 600030.SS.csv\n",
      "Download 600031.SS.csv\n",
      "Download 600036.SS.csv\n",
      "Download 600048.SS.csv\n",
      "Download 600104.SS.csv\n",
      "Download 600111.SS.csv\n",
      "Download 600196.SS.csv\n",
      "Download 600276.SS.csv\n",
      "Download 600309.SS.csv\n",
      "Download 600346.SS.csv\n",
      "Download 600406.SS.csv\n",
      "Download 600436.SS.csv\n",
      "Download 600438.SS.csv\n",
      "Download 600519.SS.csv\n",
      "Download 600585.SS.csv\n",
      "Download 600690.SS.csv\n",
      "Download 600745.SS.csv\n",
      "Download 600809.SS.csv\n",
      "Download 600887.SS.csv\n",
      "Download 600893.SS.csv\n",
      "Download 600900.SS.csv\n",
      "Download 600905.SS.csv\n",
      "Download 601012.SS.csv\n",
      "Download 601066.SS.csv\n",
      "Download 601088.SS.csv\n",
      "Download 601166.SS.csv\n",
      "Download 601225.SS.csv\n",
      "Download 601288.SS.csv\n",
      "Download 601318.SS.csv\n",
      "Download 601398.SS.csv\n",
      "Download 601601.SS.csv\n",
      "Download 601628.SS.csv\n",
      "Download 601633.SS.csv\n",
      "Download 601668.SS.csv\n",
      "Download 601669.SS.csv\n",
      "Download 601688.SS.csv\n",
      "Download 601857.SS.csv\n",
      "Download 601888.SS.csv\n",
      "Download 601899.SS.csv\n",
      "Download 601919.SS.csv\n",
      "Download 601995.SS.csv\n",
      "Download 603259.SS.csv\n",
      "Download 603260.SS.csv\n",
      "Download 603288.SS.csv\n",
      "Download 603501.SS.csv\n",
      "Download 603799.SS.csv\n",
      "Download 603986.SS.csv\n",
      "Download 688599.SS.csv\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(RAW_DATA_DIR, 'SSE50')\n",
    "check_and_make_directories(save_dir)\n",
    "retry_list = download_ticker_with_yfince(tic_list, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_list = download_ticker_with_yfince(retry_list, RAW_DATA_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download SZSE Growth 40 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\FTL\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "# download .xls from http://www.szse.cn/market/exponent/sample/index.html\n",
    "xls = pd.read_excel('./datasets/深圳成长40指数.xlsx')\n",
    "tic_list = [f'{code:06d}.SZ' for code in xls['证券代码']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 000661.SZ.csv\n",
      "Download 000725.SZ.csv\n",
      "Download 002030.SZ.csv\n",
      "Download 002049.SZ.csv\n",
      "Download 002129.SZ.csv\n",
      "Download 002271.SZ.csv\n",
      "Download 002414.SZ.csv\n",
      "Download 002460.SZ.csv\n",
      "Download 002475.SZ.csv\n",
      "Download 002555.SZ.csv\n",
      "Download 002709.SZ.csv\n",
      "Download 002714.SZ.csv\n",
      "Download 002932.SZ.csv\n",
      "Download 300014.SZ.csv\n",
      "Download 300059.SZ.csv\n",
      "Download 300122.SZ.csv\n",
      "Download 300450.SZ.csv\n",
      "Download 300502.SZ.csv\n",
      "Download 300593.SZ.csv\n",
      "Download 300604.SZ.csv\n",
      "Download 300630.SZ.csv\n",
      "Download 300638.SZ.csv\n",
      "Download 300661.SZ.csv\n",
      "Download 300671.SZ.csv\n",
      "Download 300676.SZ.csv\n",
      "Download 300677.SZ.csv\n",
      "Download 300724.SZ.csv\n",
      "Download 300763.SZ.csv\n",
      "Download 300769.SZ.csv\n",
      "Download 300772.SZ.csv\n",
      "Download 300782.SZ.csv\n",
      "Download 300850.SZ.csv\n",
      "Download 300869.SZ.csv\n",
      "Download 300888.SZ.csv\n",
      "Download 300894.SZ.csv\n",
      "Download 300896.SZ.csv\n",
      "Download 300957.SZ.csv\n",
      "Download 300973.SZ.csv\n",
      "Download 301050.SZ.csv\n",
      "Download 301080.SZ.csv\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(RAW_DATA_DIR, 'SZSEGrowth40')\n",
    "check_and_make_directories(save_dir)\n",
    "retry_list = download_ticker_with_yfince(tic_list, save_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download DOW 30 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_dict = {\n",
    "    'SZSE_Growth_40': '399326.SZ',\n",
    "    'SSE_50': '000016.SS',\n",
    "    'DOW_30': '^DJI'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399326.SZ: 1d data not available for startTime=-2208994789 and endTime=1675075747. Only 100 years worth of day granularity data are allowed to be fetched per request.\n",
      "000016.SS: 1d data not available for startTime=-2208994789 and endTime=1675075749. Only 100 years worth of day granularity data are allowed to be fetched per request.\n",
      "Download ^DJI.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['399326.SZ', '000016.SS']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = os.path.join(RAW_DATA_DIR, 'index')\n",
    "check_and_make_directories(save_dir)\n",
    "download_ticker_with_yfince(list(tic_dict.values()), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SSE 50 history from https://www.investing.com/indices/shanghai-se-50-historical-data\n",
    "# Download SZSE Growth 40 history from https://www.investing.com/indices/szse-growth-price-historical-data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download index history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calendar_with_tushare(start: str, end: str) -> pd.Series:\n",
    "    start = start.replace('-', '')\n",
    "    end = end.replace('-', '')\n",
    "\n",
    "    tushare.set_token(tushare_token)\n",
    "    tu_pro = tushare.pro_api()\n",
    "    calendar_ss = tu_pro.trade_cal(exchange='SSE', start_date=start, end_date=end, is_open=1)\n",
    "    calendar_sz = tu_pro.trade_cal(exchange='SZSE', start_date=start, end_date=end, is_open=1)\n",
    "    if calendar_ss.shape[0] != calendar_ss.shape[0]:\n",
    "        calendar = pd.merge(calendar_ss.cal_date, calendar_sz.cal_date, on=['cal_date'], how='outer')\n",
    "    else:\n",
    "        calendar = calendar_ss.cal_date\n",
    "\n",
    "    calendar = pd.to_datetime(calendar, format='%Y%m%d')\n",
    "    calendar.rename('date', inplace=True)\n",
    "    \n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_from_yfinance(data: pd.DataFrame, calendar: pd.Series = None) -> pd.DataFrame:\n",
    "    # TODO: calculate adjusted price.\n",
    "    data.drop(labels=['Dividends', 'Stock Splits'], axis='columns', inplace=True)\n",
    "    data.rename(columns={\n",
    "        'Date': 'date',\n",
    "        'Open': 'open',\n",
    "        'High': 'high',\n",
    "        'Low': 'low',\n",
    "        'Close' : 'close',\n",
    "        'Volume' : 'volume'\n",
    "        }, inplace=True)\n",
    "    data['date'] = pd.to_datetime(data['date'].apply(lambda s: s.split(' ')[0]), format='%Y-%m-%d')\n",
    "    data = pd.merge(calendar, data, how='left', on='date')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_THRESHOLD = 0.1\n",
    "\n",
    "def clean(from_dir, to_dir):\n",
    "    calendar = get_calendar_with_tushare(TRAIN_START_DAY, TRADE_END_DAY)\n",
    "\n",
    "    na_list = []\n",
    "    _, _, files = next(os.walk(from_dir))\n",
    "    for file in files:\n",
    "        result_path = os.path.join(to_dir, file)\n",
    "        if os.path.exists(result_path):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(from_dir, file)\n",
    "        df = pd.read_csv(file_path, index_col=False)\n",
    "\n",
    "        df = clean_data_from_yfinance(df, calendar)\n",
    "\n",
    "        len_df = df.shape[0]\n",
    "        df.dropna(inplace=True)\n",
    "        df.drop_duplicates(['open', 'high', 'low', 'close', 'volume'], inplace=True)\n",
    "        print(f'{len_df - df.shape[0]} rows droped from {file}.')\n",
    "\n",
    "        if df.shape[0] >= len(calendar) * (1 - NA_THRESHOLD):\n",
    "            df.to_csv(result_path, index=False)\n",
    "        else:\n",
    "            na_list.append(file)\n",
    "            print(f'{file}: too many NaNs, discard.')\n",
    "        \n",
    "    return na_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 rows droped from 600010.SS.csv.\n",
      "7 rows droped from 600028.SS.csv.\n",
      "31 rows droped from 600030.SS.csv.\n",
      "17 rows droped from 600031.SS.csv.\n",
      "22 rows droped from 600036.SS.csv.\n",
      "9 rows droped from 600048.SS.csv.\n",
      "66 rows droped from 600104.SS.csv.\n",
      "8 rows droped from 600111.SS.csv.\n",
      "20 rows droped from 600196.SS.csv.\n",
      "5 rows droped from 600276.SS.csv.\n",
      "136 rows droped from 600309.SS.csv.\n",
      "303 rows droped from 600346.SS.csv.\n",
      "142 rows droped from 600406.SS.csv.\n",
      "20 rows droped from 600436.SS.csv.\n",
      "159 rows droped from 600438.SS.csv.\n",
      "5 rows droped from 600519.SS.csv.\n",
      "16 rows droped from 600585.SS.csv.\n",
      "96 rows droped from 600690.SS.csv.\n",
      "607 rows droped from 600745.SS.csv.\n",
      "600745.SS.csv: too many NaNs, discard.\n",
      "14 rows droped from 600809.SS.csv.\n",
      "36 rows droped from 600887.SS.csv.\n",
      "372 rows droped from 600893.SS.csv.\n",
      "600893.SS.csv: too many NaNs, discard.\n",
      "368 rows droped from 600900.SS.csv.\n",
      "600900.SS.csv: too many NaNs, discard.\n",
      "3268 rows droped from 600905.SS.csv.\n",
      "600905.SS.csv: too many NaNs, discard.\n",
      "1075 rows droped from 601012.SS.csv.\n",
      "601012.SS.csv: too many NaNs, discard.\n",
      "2553 rows droped from 601066.SS.csv.\n",
      "601066.SS.csv: too many NaNs, discard.\n",
      "70 rows droped from 601088.SS.csv.\n",
      "22 rows droped from 601166.SS.csv.\n",
      "1477 rows droped from 601225.SS.csv.\n",
      "601225.SS.csv: too many NaNs, discard.\n",
      "623 rows droped from 601288.SS.csv.\n",
      "601288.SS.csv: too many NaNs, discard.\n",
      "58 rows droped from 601318.SS.csv.\n",
      "16 rows droped from 601398.SS.csv.\n",
      "10 rows droped from 601601.SS.csv.\n",
      "8 rows droped from 601628.SS.csv.\n",
      "934 rows droped from 601633.SS.csv.\n",
      "601633.SS.csv: too many NaNs, discard.\n",
      "386 rows droped from 601668.SS.csv.\n",
      "601668.SS.csv: too many NaNs, discard.\n",
      "1034 rows droped from 601669.SS.csv.\n",
      "601669.SS.csv: too many NaNs, discard.\n",
      "537 rows droped from 601688.SS.csv.\n",
      "601688.SS.csv: too many NaNs, discard.\n",
      "8 rows droped from 601857.SS.csv.\n",
      "499 rows droped from 601888.SS.csv.\n",
      "601888.SS.csv: too many NaNs, discard.\n",
      "116 rows droped from 601899.SS.csv.\n",
      "147 rows droped from 601919.SS.csv.\n",
      "3120 rows droped from 601995.SS.csv.\n",
      "601995.SS.csv: too many NaNs, discard.\n",
      "2517 rows droped from 603259.SS.csv.\n",
      "603259.SS.csv: too many NaNs, discard.\n",
      "2391 rows droped from 603260.SS.csv.\n",
      "603260.SS.csv: too many NaNs, discard.\n",
      "1482 rows droped from 603288.SS.csv.\n",
      "603288.SS.csv: too many NaNs, discard.\n",
      "2438 rows droped from 603501.SS.csv.\n",
      "603501.SS.csv: too many NaNs, discard.\n",
      "1751 rows droped from 603799.SS.csv.\n",
      "603799.SS.csv: too many NaNs, discard.\n",
      "2302 rows droped from 603986.SS.csv.\n",
      "603986.SS.csv: too many NaNs, discard.\n",
      "3025 rows droped from 688599.SS.csv.\n",
      "688599.SS.csv: too many NaNs, discard.\n",
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'600745.SS.csv 600893.SS.csv 600900.SS.csv 600905.SS.csv 601012.SS.csv 601066.SS.csv 601225.SS.csv 601288.SS.csv 601633.SS.csv 601668.SS.csv 601669.SS.csv 601688.SS.csv 601888.SS.csv 601995.SS.csv 603259.SS.csv 603260.SS.csv 603288.SS.csv 603501.SS.csv 603799.SS.csv 603986.SS.csv 688599.SS.csv'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_dir = os.path.join(RAW_DATA_DIR, 'SSE50')\n",
    "to_dir = os.path.join(CLEAN_DATA_DIR, 'SSE50')\n",
    "check_and_make_directories([from_dir, to_dir])\n",
    "na_list = clean(from_dir, to_dir)\n",
    "print('\\n', len(na_list))\n",
    "' '.join(na_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 rows droped from 000661.SZ.csv.\n",
      "21 rows droped from 000725.SZ.csv.\n",
      "58 rows droped from 002030.SZ.csv.\n",
      "278 rows droped from 002049.SZ.csv.\n",
      "464 rows droped from 002129.SZ.csv.\n",
      "002129.SZ.csv: too many NaNs, discard.\n",
      "182 rows droped from 002271.SZ.csv.\n",
      "706 rows droped from 002414.SZ.csv.\n",
      "002414.SZ.csv: too many NaNs, discard.\n",
      "715 rows droped from 002460.SZ.csv.\n",
      "002460.SZ.csv: too many NaNs, discard.\n",
      "686 rows droped from 002475.SZ.csv.\n",
      "002475.SZ.csv: too many NaNs, discard.\n",
      "1039 rows droped from 002555.SZ.csv.\n",
      "002555.SZ.csv: too many NaNs, discard.\n",
      "1497 rows droped from 002709.SZ.csv.\n",
      "002709.SZ.csv: too many NaNs, discard.\n",
      "1500 rows droped from 002714.SZ.csv.\n",
      "002714.SZ.csv: too many NaNs, discard.\n",
      "2561 rows droped from 002932.SZ.csv.\n",
      "002932.SZ.csv: too many NaNs, discard.\n",
      "495 rows droped from 300014.SZ.csv.\n",
      "300014.SZ.csv: too many NaNs, discard.\n",
      "593 rows droped from 300059.SZ.csv.\n",
      "300059.SZ.csv: too many NaNs, discard.\n",
      "739 rows droped from 300122.SZ.csv.\n",
      "300122.SZ.csv: too many NaNs, discard.\n",
      "1937 rows droped from 300450.SZ.csv.\n",
      "300450.SZ.csv: too many NaNs, discard.\n",
      "1987 rows droped from 300502.SZ.csv.\n",
      "300502.SZ.csv: too many NaNs, discard.\n",
      "2241 rows droped from 300593.SZ.csv.\n",
      "300593.SZ.csv: too many NaNs, discard.\n",
      "2270 rows droped from 300604.SZ.csv.\n",
      "300604.SZ.csv: too many NaNs, discard.\n",
      "2248 rows droped from 300630.SZ.csv.\n",
      "300630.SZ.csv: too many NaNs, discard.\n",
      "2263 rows droped from 300638.SZ.csv.\n",
      "300638.SZ.csv: too many NaNs, discard.\n",
      "2301 rows droped from 300661.SZ.csv.\n",
      "300661.SZ.csv: too many NaNs, discard.\n",
      "2313 rows droped from 300671.SZ.csv.\n",
      "300671.SZ.csv: too many NaNs, discard.\n",
      "2328 rows droped from 300676.SZ.csv.\n",
      "300676.SZ.csv: too many NaNs, discard.\n",
      "2326 rows droped from 300677.SZ.csv.\n",
      "300677.SZ.csv: too many NaNs, discard.\n",
      "2584 rows droped from 300724.SZ.csv.\n",
      "300724.SZ.csv: too many NaNs, discard.\n",
      "2728 rows droped from 300763.SZ.csv.\n",
      "300763.SZ.csv: too many NaNs, discard.\n",
      "2746 rows droped from 300769.SZ.csv.\n",
      "300769.SZ.csv: too many NaNs, discard.\n",
      "2760 rows droped from 300772.SZ.csv.\n",
      "300772.SZ.csv: too many NaNs, discard.\n",
      "2786 rows droped from 300782.SZ.csv.\n",
      "300782.SZ.csv: too many NaNs, discard.\n",
      "3046 rows droped from 300850.SZ.csv.\n",
      "300850.SZ.csv: too many NaNs, discard.\n",
      "3076 rows droped from 300869.SZ.csv.\n",
      "300869.SZ.csv: too many NaNs, discard.\n",
      "3094 rows droped from 300888.SZ.csv.\n",
      "300888.SZ.csv: too many NaNs, discard.\n",
      "3163 rows droped from 300894.SZ.csv.\n",
      "300894.SZ.csv: too many NaNs, discard.\n",
      "3101 rows droped from 300896.SZ.csv.\n",
      "300896.SZ.csv: too many NaNs, discard.\n",
      "3217 rows droped from 300957.SZ.csv.\n",
      "300957.SZ.csv: too many NaNs, discard.\n",
      "3231 rows droped from 300973.SZ.csv.\n",
      "300973.SZ.csv: too many NaNs, discard.\n",
      "3320 rows droped from 301050.SZ.csv.\n",
      "301050.SZ.csv: too many NaNs, discard.\n",
      "3352 rows droped from 301080.SZ.csv.\n",
      "301080.SZ.csv: too many NaNs, discard.\n",
      "35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'002129.SZ.csv 002414.SZ.csv 002460.SZ.csv 002475.SZ.csv 002555.SZ.csv 002709.SZ.csv 002714.SZ.csv 002932.SZ.csv 300014.SZ.csv 300059.SZ.csv 300122.SZ.csv 300450.SZ.csv 300502.SZ.csv 300593.SZ.csv 300604.SZ.csv 300630.SZ.csv 300638.SZ.csv 300661.SZ.csv 300671.SZ.csv 300676.SZ.csv 300677.SZ.csv 300724.SZ.csv 300763.SZ.csv 300769.SZ.csv 300772.SZ.csv 300782.SZ.csv 300850.SZ.csv 300869.SZ.csv 300888.SZ.csv 300894.SZ.csv 300896.SZ.csv 300957.SZ.csv 300973.SZ.csv 301050.SZ.csv 301080.SZ.csv'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_dir = os.path.join(RAW_DATA_DIR, 'SZSEGrowth40')\n",
    "to_dir = os.path.join(CLEAN_DATA_DIR, 'SZSEGrowth40')\n",
    "check_and_make_directories([from_dir, to_dir])\n",
    "na_list = clean(from_dir, to_dir)\n",
    "print('\\n', len(na_list))\n",
    "' '.join(na_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['open', 'high', 'low', 'close', 'volume', 'change', 'rs_14', 'rsi',\n",
       "       'rsi_14', 'stochrsi', 'rate', 'middle', 'tp', 'boll', 'boll_ub',\n",
       "       'boll_lb', 'macd', 'macds', 'macdh', 'ppo', 'ppos', 'ppoh', 'rsv_9',\n",
       "       'kdjk_9', 'kdjk', 'kdjd_9', 'kdjd', 'kdjj_9', 'kdjj', 'cr', 'cr-ma1',\n",
       "       'cr-ma2', 'cr-ma3', 'cci', 'tr', 'atr', 'high_delta', 'um', 'low_delta',\n",
       "       'dm', 'pdm', 'pdm_14_ema', 'pdm_14', 'atr_14', 'pdi_14', 'pdi', 'mdm',\n",
       "       'mdm_14_ema', 'mdm_14', 'mdi_14', 'mdi', 'dx_14', 'dx', 'adx', 'adxr',\n",
       "       'trix', 'tema', 'vr', 'close_10_sma', 'close_50_sma', 'dma', 'vwma',\n",
       "       'chop', 'log-ret', 'mfi', 'wt1', 'wt2', 'wr', 'supertrend_ub',\n",
       "       'supertrend_lb', 'supertrend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns after init_all()\n",
    "df = pd.read_csv('./datasets/clean/000001.SZ.csv', index_col=False)\n",
    "stats = stockstats.StockDataFrame.retype(df)\n",
    "stats.init_all()\n",
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 000001.SZ.csv.\n",
      "Added 000002.SZ.csv.\n",
      "Added 000063.SZ.csv.\n",
      "Added 000069.SZ.csv.\n",
      "Added 000100.SZ.csv.\n",
      "Added 000157.SZ.csv.\n",
      "Added 000166.SZ.csv.\n",
      "Added 000301.SZ.csv.\n",
      "Added 000338.SZ.csv.\n",
      "Added 000425.SZ.csv.\n",
      "Added 000538.SZ.csv.\n",
      "Added 000568.SZ.csv.\n",
      "Added 000596.SZ.csv.\n",
      "Added 000625.SZ.csv.\n",
      "Added 000651.SZ.csv.\n",
      "Added 000661.SZ.csv.\n",
      "Added 000708.SZ.csv.\n",
      "Added 000723.SZ.csv.\n",
      "Added 000725.SZ.csv.\n",
      "Added 000733.SZ.csv.\n",
      "Added 000768.SZ.csv.\n",
      "Added 000786.SZ.csv.\n",
      "Added 000800.SZ.csv.\n",
      "Added 000858.SZ.csv.\n",
      "Added 000876.SZ.csv.\n",
      "Added 000877.SZ.csv.\n",
      "Added 000895.SZ.csv.\n",
      "Added 000938.SZ.csv.\n",
      "Added 000963.SZ.csv.\n",
      "Added 000977.SZ.csv.\n",
      "Added 001979.SZ.csv.\n",
      "Added 002001.SZ.csv.\n",
      "Added 002007.SZ.csv.\n",
      "Added 002008.SZ.csv.\n",
      "Added 002027.SZ.csv.\n",
      "Added 002032.SZ.csv.\n",
      "Added 002049.SZ.csv.\n",
      "Added 002050.SZ.csv.\n",
      "Added 002064.SZ.csv.\n",
      "Added 002074.SZ.csv.\n",
      "Added 002120.SZ.csv.\n",
      "Added 002142.SZ.csv.\n",
      "Added 002179.SZ.csv.\n",
      "Added 002180.SZ.csv.\n",
      "Added 002202.SZ.csv.\n",
      "Added 002230.SZ.csv.\n",
      "Added 002236.SZ.csv.\n",
      "Added 002241.SZ.csv.\n",
      "Added 002271.SZ.csv.\n",
      "Added 600000.SS.csv.\n",
      "Added 600009.SS.csv.\n",
      "Added 600010.SS.csv.\n",
      "Added 600011.SS.csv.\n",
      "Added 600015.SS.csv.\n",
      "Added 600016.SS.csv.\n",
      "Added 600018.SS.csv.\n",
      "Added 600019.SS.csv.\n",
      "Added 600028.SS.csv.\n",
      "Added 600029.SS.csv.\n",
      "Added 600030.SS.csv.\n",
      "Added 600031.SS.csv.\n",
      "Added 600036.SS.csv.\n",
      "Added 600039.SS.csv.\n",
      "Added 600048.SS.csv.\n",
      "Added 600050.SS.csv.\n",
      "Added 600061.SS.csv.\n",
      "Added 600085.SS.csv.\n",
      "Added 600089.SS.csv.\n",
      "Added 600104.SS.csv.\n",
      "Added 600111.SS.csv.\n",
      "Added 600115.SS.csv.\n",
      "Added 600132.SS.csv.\n",
      "Added 600150.SS.csv.\n",
      "Added 600176.SS.csv.\n",
      "Added 600183.SS.csv.\n",
      "Added 600188.SS.csv.\n",
      "Added 600196.SS.csv.\n",
      "Added 600219.SS.csv.\n",
      "Added 600233.SS.csv.\n",
      "Added 600276.SS.csv.\n",
      "Added 600309.SS.csv.\n",
      "Added 600332.SS.csv.\n",
      "Added 600346.SS.csv.\n",
      "Added 600362.SS.csv.\n",
      "Added 600383.SS.csv.\n",
      "Added 600406.SS.csv.\n",
      "Added 600426.SS.csv.\n",
      "Added 600436.SS.csv.\n",
      "Added 600438.SS.csv.\n",
      "Added 600460.SS.csv.\n",
      "Added 600519.SS.csv.\n",
      "Added 600547.SS.csv.\n",
      "Added 600570.SS.csv.\n",
      "Added 600584.SS.csv.\n",
      "Added 600585.SS.csv.\n",
      "Added 600588.SS.csv.\n",
      "Added 600600.SS.csv.\n",
      "Added 600606.SS.csv.\n",
      "Added 600660.SS.csv.\n",
      "Added 600674.SS.csv.\n",
      "Added 600690.SS.csv.\n",
      "Added 600741.SS.csv.\n",
      "Added 600760.SS.csv.\n",
      "Added 600763.SS.csv.\n",
      "Added 600795.SS.csv.\n",
      "Added 600803.SS.csv.\n",
      "Added 600809.SS.csv.\n",
      "Added 600837.SS.csv.\n",
      "Added 600845.SS.csv.\n",
      "Added 600884.SS.csv.\n",
      "Added 600886.SS.csv.\n",
      "Added 600887.SS.csv.\n",
      "Added 601006.SS.csv.\n",
      "Added 601009.SS.csv.\n",
      "Added 601088.SS.csv.\n",
      "Added 601111.SS.csv.\n",
      "Added 601166.SS.csv.\n",
      "Added 601169.SS.csv.\n",
      "Added 601186.SS.csv.\n",
      "Added 601318.SS.csv.\n",
      "Added 601328.SS.csv.\n",
      "Added 601390.SS.csv.\n",
      "Added 601398.SS.csv.\n",
      "Added 601600.SS.csv.\n",
      "Added 601601.SS.csv.\n",
      "Added 601628.SS.csv.\n",
      "Added 601766.SS.csv.\n",
      "Added 601808.SS.csv.\n",
      "Added 601857.SS.csv.\n",
      "Added 601898.SS.csv.\n",
      "Added 601899.SS.csv.\n",
      "Added 601919.SS.csv.\n",
      "Added 601939.SS.csv.\n",
      "Added 601988.SS.csv.\n",
      "Added 601998.SS.csv.\n"
     ]
    }
   ],
   "source": [
    "X_y_filename = 'x_y.csv'\n",
    "X_y_path = os.path.join(DATA_SAVE_DIR, X_y_filename)\n",
    "\n",
    "if not os.path.exists(X_y_path):\n",
    "    X_y = None\n",
    "\n",
    "    _, _, files = next(os.walk(CLEAN_DATA_DIR))\n",
    "    for file in files:\n",
    "        file_path = os.path.join(CLEAN_DATA_DIR, file)\n",
    "        df = pd.read_csv(file_path, index_col=False)\n",
    "        stats = stockstats.StockDataFrame.retype(df)\n",
    "        stats.init_all()\n",
    "\n",
    "        # drop duplicated columns\n",
    "        stats.drop_column(['rsi', 'kdjk', 'kdjd', 'kdjj'], inplace=True)\n",
    "\n",
    "        # add additional indicators: close_14_smma, close_14_mstd, close_14_mvar,\n",
    "        # close_5_sma, wr_6, rsi_6,\n",
    "        # log differential of high, low, open and volume\n",
    "        # and log2(close / open)\n",
    "        stats['close_14_smma']; stats['close_14_mstd']; stats['close_14_mvar'];\n",
    "        stats['close_5_sma'];   stats['wr_6'];  stats['rsi_6']\n",
    "        stats['log_diff_high'] = np.log2(stats['high'] / stats['high_-1_s'])\n",
    "        stats['log_diff_low']= np.log2(stats['low'] / stats['low_-1_s'])\n",
    "        stats['log_diff_open']= np.log2(stats['open'] / stats['open_-1_s'])\n",
    "        stats['log_diff_vol']= np.log2(stats['volume'] / stats['volume_-1_s'])\n",
    "        stats['log_close/open'] = np.log2(stats['close'] / stats['open'])\n",
    "        stats.drop_column(['high_-1_s', 'low_-1_s', 'open_-1_s'], inplace=True)\n",
    "        stats['log-ret_1_s']\n",
    "        stats.rename(columns={'log-ret_1_s': 'y'}, inplace=True)\n",
    "\n",
    "        # drop date\n",
    "        stats.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # deal with nan\n",
    "        stats.dropna(inplace=True)\n",
    "\n",
    "        if X_y is None:\n",
    "            X_y = stats.copy()\n",
    "        else:\n",
    "            X_y = pd.concat([X_y, stats])\n",
    "        print(f'Add {file} to X_y.')\n",
    "\n",
    "X_y.to_csv(X_y_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_filename = 'x_y.csv'\n",
    "X_y_path = os.path.join(DATA_SAVE_DIR, X_y_filename)\n",
    "if X_y is None:\n",
    "    X_y = pd.read_csv(X_y_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "feat_selector = BorutaPy(\n",
    "    verbose=2,\n",
    "    estimator=model,\n",
    "    n_estimators='auto',\n",
    "    max_iter=10\n",
    ")\n",
    "\n",
    "X = np.array(X_y.drop(labels=['y']))\n",
    "y = np.array(X_y['y'])\n",
    "feat_selector.fit(X, y)\n",
    "\n",
    "# print support and ranking for each feature\n",
    "print(\"\\n------Support and Ranking for each feature------\")\n",
    "for i in range(len(feat_selector.support_)):\n",
    "    if feat_selector.support_[i]:\n",
    "        print(\"Passes the test: \", X.columns[i],\n",
    "              \" - Ranking: \", feat_selector.ranking_[i])\n",
    "    else:\n",
    "        print(\"Doesn't pass the test: \",\n",
    "              X.columns[i], \" - Ranking: \", feat_selector.ranking_[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Features:\n",
    "* volume\n",
    "* ppo\n",
    "* cr-ma3\n",
    "* trix \n",
    "* log_diff_high\n",
    "* log_diff_low\n",
    "* log_diff_open\n",
    "* log_close/open"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(from_dir, to_dir):\n",
    "    df_dict = {}\n",
    "    _, _, files = next(os.walk(from_dir))\n",
    "    for file in files:\n",
    "        # skip if already exists\n",
    "        processed_file_path = os.path.join(to_dir, file)\n",
    "        if os.path.exists(processed_file_path):\n",
    "            continue\n",
    "        \n",
    "        # load\n",
    "        clean_file_path = os.path.join(from_dir, file)\n",
    "        stats = pd.read_csv(clean_file_path, index_col=False)\n",
    "        df = pd.DataFrame(index=stats['date'])\n",
    "\n",
    "        stats = stockstats.StockDataFrame.retype(stats)\n",
    "        df['change'] = stats['change']\n",
    "\n",
    "        # add indicators\n",
    "        df['ppo'] = stats['ppo']\n",
    "        df['cr-ma3'] = stats['cr-ma3']\n",
    "        df['trix'] = stats['trix']\n",
    "\n",
    "        # add differential features\n",
    "        df['log_close/open'] = np.log2(stats['close'] / stats['open'])\n",
    "        df['log-ret'] = stats['log-ret']\n",
    "        df['log_diff_high'] = np.log2(stats['high'] / stats['high_-1_s'])\n",
    "        df['log_diff_low'] = np.log2(stats['low'] / stats['low_-1_s'])\n",
    "        df['log_diff_open'] = np.log2(stats['open'] / stats['open_-1_s'])\n",
    "\n",
    "        # clean\n",
    "        df.dropna(inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # save\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        tic = file.split('.')[0]\n",
    "        df_dict[tic] = df.copy()\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_dir = os.path.join(CLEAN_DATA_DIR, 'SSE50')\n",
    "to_dir = os.path.join(PREPROCESSED_DATA_DIR, 'SSE50')\n",
    "check_and_make_directories(to_dir)\n",
    "df_dict_SSE50 = preprocess(from_dir, to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_dir = os.path.join(CLEAN_DATA_DIR, 'SZSEGrowth40')\n",
    "to_dir = os.path.join(PREPROCESSED_DATA_DIR, 'SZSEGrowth40')\n",
    "check_and_make_directories(to_dir)\n",
    "df_dict_SZSE40 = preprocess(from_dir, to_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(PREPROCESSED_DATA_DIR, 'SSE50')\n",
    "# Just load data\n",
    "if True or 'df_dict' not in locals():\n",
    "    df_dict = {}\n",
    "    _, _, files = next(os.walk(dataset_dir))\n",
    "    for file in files:\n",
    "        processed_file_path = os.path.join(dataset_dir, file)   \n",
    "        df = pd.read_csv(processed_file_path, index_col=False)\n",
    "        assert df.isna().sum().sum() == 0, f'Nan found in {file}.'\n",
    "        tic = file.replace('.csv', '')\n",
    "        df_dict[tic] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "df_dict_train = dict()\n",
    "df_dict_test = dict()\n",
    "df_dict_trade = dict()\n",
    "\n",
    "TEST_START_DAY = pd.to_datetime(TEST_START_DAY, format='%Y-%m-%d')\n",
    "TRADE_START_DAY = pd.to_datetime(TRADE_START_DAY, format='%Y-%m-%d')\n",
    "\n",
    "for tic, df in df_dict.items():\n",
    "    df.date = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "    df_dict_train[tic] = df.loc[df.date < TEST_START_DAY].sort_index(ascending=True).copy()\n",
    "    df_dict_test[tic] = df.loc[(df.date >= TEST_START_DAY) & (df.date < TRADE_START_DAY)].sort_index(ascending=True).copy()\n",
    "    df_dict_trade[tic] = df.loc[df.date >= TRADE_START_DAY].sort_index(ascending=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_envs(n_tickers: int = 10) -> Tuple[MultiStockTradingEnv, MultiStockTradingEnv, MultiStockTradingEnv]:\n",
    "    assert n_tickers <= len(df_dict_train)\n",
    "\n",
    "    env_list = list()\n",
    "    tic_list = random.sample(df_dict_train.keys(), n_tickers)\n",
    "    for _df_dict in [df_dict_train, df_dict_test, df_dict_trade]:\n",
    "        _dfs = list()\n",
    "        for tic in tic_list:\n",
    "            _df = _df_dict[tic]\n",
    "            _df['tic'] = tic\n",
    "            _dfs.append(_df)\n",
    "        _dfs = pd.concat(_dfs)\n",
    "        # drop dates that missing data\n",
    "        _dfs = _dfs.pivot_table(df, index=['date'], columns=['tic']).dropna().stack().reset_index()\n",
    "        _dfs.sort_values(['date', 'tic'], inplace=True)\n",
    "        _dfs.set_index(['date', 'tic'], inplace=True)\n",
    "        env_list.append(Monitor(MultiStockTradingEnv(_dfs)))\n",
    "    \n",
    "    return tuple(env_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(\n",
    "    model_name: str, \n",
    "    model_class: Type[BaseAlgorithm], \n",
    "    sample_param_func: Callable[[optuna.Trial], Tuple[Dict, int]],\n",
    "    ) -> Callable[[optuna.Trial], float]:\n",
    "    \n",
    "    def objective(trial: optuna.Trial):\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "        model_path = os.path.join(model_path, f'trial_{trial.number}_best_model')\n",
    "        tb_log_path = os.path.join(TENSORBOARD_LOG_DIR, model_name)\n",
    "        check_and_make_directories([model_path, tb_log_path])\n",
    "\n",
    "        # Create model with sampled hyperparameters and \n",
    "        # train it with early stop callback    \n",
    "        hyperparameters, total_timesteps = sample_param_func(trial)\n",
    "        hyperparameters['tensorboard_log'] = '/root/tf-logs' # tb_log_path # or \n",
    "\n",
    "        env_train, env_test, _ =  get_envs()\n",
    "        model = model_class('MlpPolicy', env_train, **hyperparameters)\n",
    "\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(\n",
    "            max_no_improvement_evals=4, min_evals=2, verbose=VERBOSE)\n",
    "        eval_callback = EvalCallback(\n",
    "            env_test, \n",
    "            callback_after_eval=stop_train_callback,\n",
    "            n_eval_episodes=3,\n",
    "            eval_freq=10000,\n",
    "            best_model_save_path=model_path, \n",
    "            verbose=VERBOSE\n",
    "            )\n",
    "\n",
    "        # deal with gradient explosion\n",
    "        try:\n",
    "            model.learn(total_timesteps=total_timesteps, \n",
    "                tb_log_name=f'{model_name}_{trial.number}', callback=eval_callback)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return -99\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "            return -99\n",
    "\n",
    "        # validation\n",
    "        mean_reward, _ = evaluate_policy(model, env_test, n_eval_episodes=3)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(\n",
    "    model_name: str, \n",
    "    model_class: Type[BaseAlgorithm],\n",
    "    sample_param_func: Callable[[optuna.Trial], Any],\n",
    "    n_trials: int = 100, \n",
    "    callbacks: List[Callable] = None\n",
    "    ) -> optuna.Study:\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=None)\n",
    "    objective = objective_factory(model_name, model_class, sample_param_func)\n",
    "\n",
    "    study_name = f'{model_name}_study'\n",
    "    storage_name = f'sqlite:///{study_name}.db'\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name, \n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "        pruner=optuna.pruners.HyperbandPruner(),\n",
    "        storage=storage_name\n",
    "        )\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=n_trials,\n",
    "        callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try to avoid CUDA OOM\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-04 09:31:17,409]\u001b[0m A new study created in RDB with name: SAC_study\u001b[0m\n",
      "/tmp/ipykernel_540722/931191452.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  tic_list = random.sample(df_dict_train.keys(), n_tickers)\n"
     ]
    }
   ],
   "source": [
    "# study_A2C = tune('A2C', A2C, \\\n",
    "#     sample_param_func=sample_a2c_param)\n",
    "\n",
    "# plot_optimization_history(study_A2C)\n",
    "# plot_param_importances(study_A2C)\n",
    "\n",
    "study_SAC = tune('SAC', SAC, \\\n",
    "    sample_param_func=sample_sac_param)\n",
    "\n",
    "plot_optimization_history(study_SAC)\n",
    "plot_param_importances(study_SAC)\n",
    "\n",
    "# study_TQC = tune('TQC', TQC, \\\n",
    "#     sample_param_func=sample_tqc_param)\n",
    "\n",
    "# plot_optimization_history(study_TQC)\n",
    "# plot_param_importances(study_TQC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with strict condition\n",
    "early_stop_callback = PruneCallback(\n",
    "    threshold=1,\n",
    "    patience=1,\n",
    "    trial_number=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-04 09:19:10,566]\u001b[0m Using an existing study with name 'TQC_study' instead of creating a new one.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study_TQC = optuna.create_study(\n",
    "    study_name='TQC_study', \n",
    "    direction='maximize', \n",
    "    sampler=optuna.samplers.TPESampler(seed=None),\n",
    "    pruner=optuna.pruners.HyperbandPruner(),\n",
    "    storage='sqlite:///TQC_study.db',\n",
    "    load_if_exists=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7efa7a793c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.a2c import A2C\n",
    "env_train, env_test, env_trade = get_envs()\n",
    "model = A2C('MlpPolicy', env_train)\n",
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_t = dfs_test[3]\n",
    "# list_asset, actions = simulate_trading_masked(env_factory([df_t]), model)\n",
    "# sr_asset = pd.Series(list_asset)\n",
    "# sr_return = get_daily_return(sr_asset)\n",
    "# backtest_stats(sr_return)\n",
    "# sr_baseline_return = get_daily_return(df_t.close).dropna()\n",
    "# sr_baseline_return = sr_baseline_return[len(sr_baseline_return) - len(sr_asset):]\n",
    "# backtest_stats(sr_baseline_return)\n",
    "# %matplotlib inline\n",
    "# sr_date = df_t.date\n",
    "# sr_date = sr_date[len(sr_date) - len(sr_asset):]\n",
    "# sr_return.set_axis(sr_date, inplace=True)\n",
    "# sr_baseline_return.set_axis(sr_date, inplace=True)\n",
    "# backtest_plot(sr_return, sr_baseline_return)\n",
    "# sum(actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "606c65d0c67f4fb4d4dcc32359b236efe9c56120e1c34efc12f6c8d1f87ab33f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
