{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/python3.9/lib/python3.9/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import typing\n",
    "from typing import Any, Callable, Dict, Type\n",
    "import warnings\n",
    "\n",
    "from boruta import BorutaPy\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_contour, plot_edf, \\\n",
    "    plot_intermediate_values, plot_optimization_history, plot_parallel_coordinate, \\\n",
    "    plot_param_importances, plot_slice\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sb3_contrib.tqc import TQC\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "from stable_baselines3.a2c import A2C\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.sac import SAC\n",
    "import stockstats\n",
    "import torch as th\n",
    "import tushare\n",
    "import yfinance as yf\n",
    "\n",
    "from environment.MultiStockTradingEnv import MultiStockTradingEnv\n",
    "from quantile_critic.tqci import TQCI\n",
    "from quantile_critic.tqcr import TQCR\n",
    "from utils.sample_funcs import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "TRAIN_START_DAY = '2008-01-01'\n",
    "TRAIN_END_DAY = '2016-12-31'\n",
    "TEST_START_DAY = '2017-01-01'\n",
    "TEST_END_DAY = '2019-12-31'\n",
    "TRADE_START_DAY = '2020-01-01'\n",
    "TRADE_END_DAY = '2022-12-31'\n",
    "\n",
    "# Setup directories\n",
    "DATA_SAVE_DIR = 'datasets'\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_TRAINED_DIR = os.path.join(MODEL_DIR, 'trained')\n",
    "TENSORBOARD_LOG_DIR = 'tensorboard_log'\n",
    "RAW_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'raw')\n",
    "RAW_DATA_WITHIN_RANGE_DIR = os.path.join(DATA_SAVE_DIR, f'raw_{TRAIN_START_DAY}_{TRADE_END_DAY}')\n",
    "CLEAN_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'clean')\n",
    "PREPROCESSED_DATA_DIR = os.path.join(DATA_SAVE_DIR, 'preprocessed')\n",
    "PREPROCESSED_SSE50_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'SSE50')\n",
    "PREPROCESSED_HSI_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'HSI')\n",
    "PREPROCESSED_DJIA_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'DJIA')\n",
    "PREPROCESSED_DAX30_DIR = os.path.join(PREPROCESSED_DATA_DIR, 'DAX30')\n",
    "\n",
    "\n",
    "check_and_make_directories([\n",
    "     DATA_SAVE_DIR, MODEL_DIR, MODEL_TRAINED_DIR, TENSORBOARD_LOG_DIR, \\\n",
    "     RAW_DATA_DIR, RAW_DATA_WITHIN_RANGE_DIR, CLEAN_DATA_DIR, PREPROCESSED_DATA_DIR, \\\n",
    "     PREPROCESSED_SSE50_DIR, PREPROCESSED_HSI_DIR, PREPROCESSED_DJIA_DIR, PREPROCESSED_DAX30_DIR\n",
    "     ])\n",
    "\n",
    "tushare_token = '2bf5fdb105eefda26ef27cc9caa94e6f31ca66e408f7cc54d4fce032'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prerocess data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve SSE 50 component list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE 50 components from http://www.sse.com.cn/market/sseindex/indexlist/basic/index.shtml?COMPANY_CODE=000016&INDEX_Code=000016&type=1\n",
    "SSE50_COM = \\\n",
    "\"\"\"\n",
    "包钢股份(600010)\t中国石化(600028)\t中信证券(600030)\n",
    "三一重工(600031)\t招商银行(600036)\t保利发展(600048)\n",
    "上汽集团(600104)\t北方稀土(600111)\t复星医药(600196)\n",
    "恒瑞医药(600276)\t万华化学(600309)\t恒力石化(600346)\n",
    "国电南瑞(600406)\t片仔癀(600436)\t    通威股份(600438)\n",
    "贵州茅台(600519)\t海螺水泥(600585)\t海尔智家(600690)\n",
    "闻泰科技(600745)\t山西汾酒(600809)\t伊利股份(600887)\n",
    "航发动力(600893)\t长江电力(600900)\t三峡能源(600905)\n",
    "隆基绿能(601012)\t中信建投(601066)\t中国神华(601088)\n",
    "兴业银行(601166)\t陕西煤业(601225)\t农业银行(601288)\n",
    "中国平安(601318)\t工商银行(601398)\t中国太保(601601)\n",
    "中国人寿(601628)\t长城汽车(601633)\t中国建筑(601668)\n",
    "中国电建(601669)\t华泰证券(601688)\t中国石油(601857)\n",
    "中国中免(601888)\t紫金矿业(601899)\t中远海控(601919)\n",
    "中金公司(601995)\t药明康德(603259)\t合盛硅业(603260)\n",
    "海天味业(603288)\t韦尔股份(603501)\t华友钴业(603799)\n",
    "兆易创新(603986)\t天合光能(688599)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_list = re.findall(r'\\d{6}', SSE50_COM)\n",
    "tic_list = [tic+'.SS' for tic in tic_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download SSE50 ticker history with yfinace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_with_yfince(tic_list: List[str], download_dir: str) -> List[str]:\n",
    "    retry_list = []\n",
    "    for tic in tic_list:\n",
    "        if tic.startswith('CON'):\n",
    "            csv_path = os.path.join(download_dir, f'_{tic}.csv')\n",
    "        else:\n",
    "            csv_path = os.path.join(download_dir, f'{tic}.csv')\n",
    "            \n",
    "        if os.path.exists(csv_path):\n",
    "            print(f'File {csv_path} already exist. Skip')\n",
    "            continue\n",
    "        \n",
    "        ticker = yf.Ticker(tic)\n",
    "        df = ticker.history(period='max')\n",
    "        if df.shape[0] > 0:\n",
    "            df.to_csv(csv_path)\n",
    "            print(f'Download {tic}.csv')         \n",
    "            time.sleep(0.1)\n",
    "        else:\n",
    "            retry_list.append(tic)\n",
    "    \n",
    "    return retry_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File datasets\\raw\\600010.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600028.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600030.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600031.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600036.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600048.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600104.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600111.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600196.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600276.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600309.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600346.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600406.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600436.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600438.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600519.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600585.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600690.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600745.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600809.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600887.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600893.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600900.SS.csv already exist. Skip\n",
      "File datasets\\raw\\600905.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601012.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601066.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601088.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601166.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601225.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601288.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601318.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601398.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601601.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601628.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601633.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601668.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601669.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601688.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601857.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601888.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601899.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601919.SS.csv already exist. Skip\n",
      "File datasets\\raw\\601995.SS.csv already exist. Skip\n",
      "File datasets\\raw\\603259.SS.csv already exist. Skip\n",
      "File datasets\\raw\\603260.SS.csv already exist. Skip\n",
      "File datasets\\raw\\603288.SS.csv already exist. Skip\n",
      "File datasets\\raw\\603501.SS.csv already exist. Skip\n",
      "File datasets\\raw\\603799.SS.csv already exist. Skip\n",
      "File datasets\\raw\\603986.SS.csv already exist. Skip\n",
      "File datasets\\raw\\688599.SS.csv already exist. Skip\n"
     ]
    }
   ],
   "source": [
    "retry_list = download_ticker_with_yfince(tic_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_list = download_ticker_with_yfince(retry_list, RAW_DATA_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data with adjusted price from yfinace within range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker_with_yfince_within_range(tic_list, start, end):\n",
    "    for tic in tic_list:\n",
    "        if tic.startswith('CON'):\n",
    "            csv_path = os.path.join(RAW_DATA_DIR, f'_{tic}.csv')\n",
    "        else:\n",
    "            csv_path = os.path.join(RAW_DATA_DIR, f'{tic}.csv')\n",
    "\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path, index_col=False)\n",
    "            first_day = df['Date'].iloc[0].split(' ')[0]\n",
    "            last_day = df['Date'].iloc[-1].split(' ')[0]\n",
    "            if first_day < start and last_day > end:\n",
    "                if tic.startswith('CON'):\n",
    "                    csv_path = os.path.join(RAW_DATA_WITHIN_RANGE_DIR, f'_{tic}.csv')\n",
    "                else:\n",
    "                    csv_path = os.path.join(RAW_DATA_WITHIN_RANGE_DIR, f'{tic}.csv')\n",
    "                if not os.path.exists(csv_path):\n",
    "                    df_within_range = yf.download(tic, start=start, end=end)\n",
    "                    df_within_range.to_csv(csv_path)\n",
    "            else:\n",
    "                print(f'{tic}: from {first_day} to {last_day} out of range.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_ticker_with_yfince_within_range(tic_list, TRAIN_START_DAY, TRADE_END_DAY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_THRESHOLD = 0.1\n",
    "FEATURES = 'kdjj_9 volume change stochrsi ppoh ppo ppos dx_14 cr-ma3 adxr'\n",
    "\n",
    "def preprocess_data(tic_list, load_dir, save_dir):\n",
    "    na_list = []\n",
    "    for tic in tic_list: \n",
    "        if tic.startswith('CON'):\n",
    "            csv_path = os.path.join(load_dir, f'_{tic}.csv')\n",
    "        else:\n",
    "            csv_path = os.path.join(load_dir, f'{tic}.csv')\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f'File {csv_path} doesn\\'t exist. Skip')\n",
    "            continue\n",
    "        data = pd.read_csv(csv_path, index_col=False)\n",
    "\n",
    "        # rename columns and drop 'close'\n",
    "        data.drop(labels=['Close'], axis='columns', inplace=True)\n",
    "        data.rename(columns={\n",
    "            'Date': 'date',\n",
    "            'Open': 'open',\n",
    "            'High': 'high',\n",
    "            'Low': 'low',\n",
    "            'Adj Close' : 'close',\n",
    "            'Volume' : 'volume'\n",
    "            }, inplace=True)\n",
    "        data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "\n",
    "        # clean data\n",
    "        len_df = data.shape[0]\n",
    "        data.drop_duplicates(subset=['open', 'high', 'low', 'close', 'volume'], inplace=True)\n",
    "        data.dropna(inplace=True)\n",
    "        print(f'{len_df - data.shape[0]} rows droped from ticker {tic}.')\n",
    "\n",
    "        if data.shape[0] < len_df * (1 - NA_THRESHOLD):\n",
    "            na_list.append(tic)\n",
    "            print(f'{tic}: too many NaNs, discard.')\n",
    "            continue\n",
    "\n",
    "        # add features\n",
    "        df = pd.DataFrame(index=data['date'])\n",
    "        data = stockstats.StockDataFrame.retype(data)\n",
    "\n",
    "        feature_list = FEATURES.split(' ')\n",
    "        for f in feature_list:\n",
    "            df[f] = data[f]\n",
    "\n",
    "        # Drop nan, inf\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        assert df.isna().sum().sum() == 0\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # save data\n",
    "        if tic.startswith('CON'):\n",
    "            result_path = os.path.join(save_dir, f'_{tic}.csv')\n",
    "        else:\n",
    "            result_path = os.path.join(save_dir, f'{tic}.csv')\n",
    "\n",
    "        df.to_csv(result_path, index=False)\n",
    "\n",
    "    return na_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 rows droped from ticker 600010.SS.\n",
      "0 rows droped from ticker 600028.SS.\n",
      "28 rows droped from ticker 600030.SS.\n",
      "3 rows droped from ticker 600031.SS.\n",
      "9 rows droped from ticker 600036.SS.\n",
      "4 rows droped from ticker 600048.SS.\n",
      "56 rows droped from ticker 600104.SS.\n",
      "0 rows droped from ticker 600111.SS.\n",
      "12 rows droped from ticker 600196.SS.\n",
      "3 rows droped from ticker 600276.SS.\n",
      "129 rows droped from ticker 600309.SS.\n",
      "301 rows droped from ticker 600346.SS.\n",
      "140 rows droped from ticker 600406.SS.\n",
      "9 rows droped from ticker 600436.SS.\n",
      "140 rows droped from ticker 600438.SS.\n",
      "0 rows droped from ticker 600519.SS.\n",
      "7 rows droped from ticker 600585.SS.\n",
      "84 rows droped from ticker 600690.SS.\n",
      "477 rows droped from ticker 600745.SS.\n",
      "600745.SS: too many NaNs, discard.\n",
      "12 rows droped from ticker 600809.SS.\n",
      "28 rows droped from ticker 600887.SS.\n",
      "124 rows droped from ticker 600893.SS.\n",
      "161 rows droped from ticker 600900.SS.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\600905.SS.csv doesn't exist. Skip\n",
      "35 rows droped from ticker 601012.SS.\n",
      "1 rows droped from ticker 601066.SS.\n",
      "64 rows droped from ticker 601088.SS.\n",
      "10 rows droped from ticker 601166.SS.\n",
      "0 rows droped from ticker 601225.SS.\n",
      "0 rows droped from ticker 601288.SS.\n",
      "50 rows droped from ticker 601318.SS.\n",
      "5 rows droped from ticker 601398.SS.\n",
      "1 rows droped from ticker 601601.SS.\n",
      "3 rows droped from ticker 601628.SS.\n",
      "19 rows droped from ticker 601633.SS.\n",
      "0 rows droped from ticker 601668.SS.\n",
      "110 rows droped from ticker 601669.SS.\n",
      "6 rows droped from ticker 601688.SS.\n",
      "2 rows droped from ticker 601857.SS.\n",
      "60 rows droped from ticker 601888.SS.\n",
      "27 rows droped from ticker 601899.SS.\n",
      "145 rows droped from ticker 601919.SS.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\601995.SS.csv doesn't exist. Skip\n",
      "0 rows droped from ticker 603259.SS.\n",
      "0 rows droped from ticker 603260.SS.\n",
      "0 rows droped from ticker 603288.SS.\n",
      "167 rows droped from ticker 603501.SS.\n",
      "603501.SS: too many NaNs, discard.\n",
      "29 rows droped from ticker 603799.SS.\n",
      "200 rows droped from ticker 603986.SS.\n",
      "603986.SS: too many NaNs, discard.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\688599.SS.csv doesn't exist. Skip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['600745.SS', '603501.SS', '603986.SS']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data(tic_list, RAW_DATA_WITHIN_RANGE_DIR, PREPROCESSED_SSE50_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from https://www.hsi.com.hk/chi/indexes/all-indexes/hsi\n",
    "HSI_components = '''\n",
    "5\t滙豐控股\t+6\n",
    "11\t恒生銀行\t0\n",
    "388\t香港交易所\t+12\n",
    "939\t建設銀行\t-2\n",
    "1299\t友邦保險\t+41\n",
    "1398\t工商銀行\t-4\n",
    "2318\t中國平安\t+16\n",
    "2388\t中銀香港\t0\n",
    "2628\t中國人壽\t+6\n",
    "3968\t招商銀行\t0\n",
    "3988\t中國銀行\t+1\n",
    "2\t中電控股\t0\n",
    "3\t香港中華煤氣\t0\n",
    "6\t電能實業\t0\n",
    "1038\t長江基建集團\t0\n",
    "2688\t新奧能源\t0\n",
    "12\t恒基地產\t0\n",
    "16\t新鴻基地產\t-1\n",
    "17\t新世界發展\t+1\n",
    "101\t恒隆地產\t0\n",
    "688\t中國海外發展\t0\n",
    "823\t領展房產基金\t-2\n",
    "960\t龍湖集團\t+2\n",
    "1109\t華潤置地\t+1\n",
    "1113\t長實集團\t0\n",
    "1209\t華潤萬象生活\t+1\n",
    "1997\t九龍倉置業\t0\n",
    "2007\t碧桂園\t+1\n",
    "6098\t碧桂園服務\t+1\n",
    "1\t長和\t+2\n",
    "27\t銀河娛樂\t+6\n",
    "66\t港鐵公司\t0\n",
    "175\t吉利汽車\t+3\n",
    "241\t阿里健康\t+2\n",
    "267\t中信股份\t-1\n",
    "288\t萬洲國際\t0\n",
    "291\t華潤啤酒\t+5\n",
    "316\t東方海外國際\t0\n",
    "322\t康師傅控股\t0\n",
    "386\t中國石油化工股份\t0\n",
    "669\t創科實業\t+2\n",
    "700\t騰訊控股\t+69\n",
    "762\t中國聯通\t+1\n",
    "857\t中國石油股份\t0\n",
    "868\t信義玻璃\t+1\n",
    "881\t中升控股\t+1\n",
    "883\t中國海洋石油\t-3\n",
    "941\t中國移動\t+4\n",
    "968\t信義光能\t+2\n",
    "981\t中芯國際\t+4\n",
    "992\t聯想集團\t+2\n",
    "1044\t恒安國際\t0\n",
    "1088\t中國神華\t0\n",
    "1093\t石藥集團\t0\n",
    "1177\t中國生物製藥\t+1\n",
    "1211\t比亞迪股份\t+11\n",
    "1378\t中國宏橋\t0\n",
    "1810\t小米集團 - W\t+30\n",
    "1876\t百威亞太\t+1\n",
    "1928\t金沙中國有限公司\t+7\n",
    "1929\t周大福\t0\n",
    "2020\t安踏體育\t+4\n",
    "2269\t藥明生物\t+3\n",
    "2313\t申洲國際\t+1\n",
    "2319\t蒙牛乳業\t0\n",
    "2331\t李寧\t+7\n",
    "2382\t舜宇光學科技\t+7\n",
    "3690\t美團 - W\t+3\n",
    "3692\t翰森製藥\t+1\n",
    "6690\t海爾智家\t+2\n",
    "6862\t海底撈\t+2\n",
    "9618\t京東集團 - SW\t+15\n",
    "9633\t農夫山泉\t-2\n",
    "9888\t百度集團 - SW\t-6\n",
    "9988\t阿里巴巴 - SW\t+72\n",
    "9999\t網易 - S\t+1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_list = [tic_row.split('\\t')[0] for tic_row in HSI_components.strip().split('\\n')]\n",
    "tic_list = [f'{int(tic):04d}.HK' for tic in tic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 0005.HK.csv\n",
      "Download 0011.HK.csv\n",
      "Download 0388.HK.csv\n",
      "Download 0939.HK.csv\n",
      "Download 1299.HK.csv\n",
      "Download 1398.HK.csv\n",
      "Download 2318.HK.csv\n",
      "Download 2388.HK.csv\n",
      "2628.HK: No data found for this date range, symbol may be delisted\n",
      "Download 3968.HK.csv\n",
      "Download 3988.HK.csv\n",
      "Download 0002.HK.csv\n",
      "Download 0003.HK.csv\n",
      "Download 0006.HK.csv\n",
      "Download 1038.HK.csv\n",
      "Download 2688.HK.csv\n",
      "Download 0012.HK.csv\n",
      "Download 0016.HK.csv\n",
      "Download 0017.HK.csv\n",
      "Download 0101.HK.csv\n",
      "Download 0688.HK.csv\n",
      "Download 0823.HK.csv\n",
      "Download 0960.HK.csv\n",
      "Download 1109.HK.csv\n",
      "Download 1113.HK.csv\n",
      "Download 1209.HK.csv\n",
      "Download 1997.HK.csv\n",
      "Download 2007.HK.csv\n",
      "Download 6098.HK.csv\n",
      "Download 0001.HK.csv\n",
      "Download 0027.HK.csv\n",
      "Download 0066.HK.csv\n",
      "Download 0175.HK.csv\n",
      "Download 0241.HK.csv\n",
      "Download 0267.HK.csv\n",
      "Download 0288.HK.csv\n",
      "Download 0291.HK.csv\n",
      "Download 0316.HK.csv\n",
      "Download 0322.HK.csv\n",
      "Download 0386.HK.csv\n",
      "Download 0669.HK.csv\n",
      "Download 0700.HK.csv\n",
      "Download 0762.HK.csv\n",
      "Download 0857.HK.csv\n",
      "Download 0868.HK.csv\n",
      "Download 0881.HK.csv\n",
      "Download 0883.HK.csv\n",
      "Download 0941.HK.csv\n",
      "Download 0968.HK.csv\n",
      "Failed to get ticker '0981.HK' reason: HTTPSConnectionPool(host='query2.finance.yahoo.com', port=443): Max retries exceeded with url: /v8/finance/chart/0981.HK?range=1d&interval=1d (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
      "- 0981.HK: No timezone found, symbol may be delisted\n",
      "Download 0992.HK.csv\n",
      "Download 1044.HK.csv\n",
      "Download 1088.HK.csv\n",
      "Download 1093.HK.csv\n",
      "Download 1177.HK.csv\n",
      "Download 1211.HK.csv\n",
      "Download 1378.HK.csv\n",
      "Download 1810.HK.csv\n",
      "Download 1876.HK.csv\n",
      "Download 1928.HK.csv\n",
      "Download 1929.HK.csv\n",
      "Download 2020.HK.csv\n",
      "Download 2269.HK.csv\n",
      "Download 2313.HK.csv\n",
      "Download 2319.HK.csv\n",
      "Download 2331.HK.csv\n",
      "Download 2382.HK.csv\n",
      "Download 3690.HK.csv\n",
      "Download 3692.HK.csv\n",
      "Download 6690.HK.csv\n",
      "Download 6862.HK.csv\n",
      "Download 9618.HK.csv\n",
      "Download 9633.HK.csv\n",
      "Download 9888.HK.csv\n",
      "Download 9988.HK.csv\n",
      "Download 9999.HK.csv\n"
     ]
    }
   ],
   "source": [
    "retry_list = download_ticker_with_yfince(tic_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 2628.HK.csv\n",
      "Download 0981.HK.csv\n"
     ]
    }
   ],
   "source": [
    "retry_list = download_ticker_with_yfince(retry_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "1299.HK: from 2010-10-29 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "0960.HK: from 2009-11-19 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "1113.HK: from 2015-06-03 to 2023-02-09 out of range.\n",
      "1209.HK: from 2020-12-09 to 2023-02-09 out of range.\n",
      "1997.HK: from 2017-11-15 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "6098.HK: from 2018-06-19 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "0288.HK: from 2014-08-05 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "0881.HK: from 2010-03-26 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "0968.HK: from 2013-12-12 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "1378.HK: from 2011-03-24 to 2023-02-09 out of range.\n",
      "1810.HK: from 2018-07-09 to 2023-02-09 out of range.\n",
      "1876.HK: from 2019-09-30 to 2023-02-09 out of range.\n",
      "1928.HK: from 2009-11-30 to 2023-02-09 out of range.\n",
      "1929.HK: from 2011-12-15 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "2269.HK: from 2017-06-13 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "3690.HK: from 2018-09-20 to 2023-02-09 out of range.\n",
      "3692.HK: from 2019-06-14 to 2023-02-09 out of range.\n",
      "6690.HK: from 2020-12-23 to 2023-02-09 out of range.\n",
      "6862.HK: from 2018-09-26 to 2023-02-09 out of range.\n",
      "9618.HK: from 2020-06-18 to 2023-02-09 out of range.\n",
      "9633.HK: from 2020-09-08 to 2023-02-09 out of range.\n",
      "9888.HK: from 2021-03-23 to 2023-02-09 out of range.\n",
      "9988.HK: from 2019-11-26 to 2023-02-09 out of range.\n",
      "9999.HK: from 2020-06-11 to 2023-02-09 out of range.\n"
     ]
    }
   ],
   "source": [
    "download_ticker_with_yfince_within_range(tic_list, TRAIN_START_DAY, TRADE_END_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows droped from ticker 0005.HK.\n",
      "0 rows droped from ticker 0011.HK.\n",
      "0 rows droped from ticker 0388.HK.\n",
      "0 rows droped from ticker 0939.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1299.HK.csv doesn't exist. Skip\n",
      "0 rows droped from ticker 1398.HK.\n",
      "4 rows droped from ticker 2318.HK.\n",
      "0 rows droped from ticker 2388.HK.\n",
      "0 rows droped from ticker 2628.HK.\n",
      "3 rows droped from ticker 3968.HK.\n",
      "0 rows droped from ticker 3988.HK.\n",
      "0 rows droped from ticker 0002.HK.\n",
      "1 rows droped from ticker 0003.HK.\n",
      "0 rows droped from ticker 0006.HK.\n",
      "0 rows droped from ticker 1038.HK.\n",
      "4 rows droped from ticker 2688.HK.\n",
      "0 rows droped from ticker 0012.HK.\n",
      "1 rows droped from ticker 0016.HK.\n",
      "3 rows droped from ticker 0017.HK.\n",
      "0 rows droped from ticker 0101.HK.\n",
      "0 rows droped from ticker 0688.HK.\n",
      "0 rows droped from ticker 0823.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\0960.HK.csv doesn't exist. Skip\n",
      "1 rows droped from ticker 1109.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1113.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1209.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1997.HK.csv doesn't exist. Skip\n",
      "0 rows droped from ticker 2007.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\6098.HK.csv doesn't exist. Skip\n",
      "4 rows droped from ticker 0001.HK.\n",
      "0 rows droped from ticker 0027.HK.\n",
      "0 rows droped from ticker 0066.HK.\n",
      "4 rows droped from ticker 0175.HK.\n",
      "19 rows droped from ticker 0241.HK.\n",
      "3 rows droped from ticker 0267.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\0288.HK.csv doesn't exist. Skip\n",
      "9 rows droped from ticker 0291.HK.\n",
      "38 rows droped from ticker 0316.HK.\n",
      "0 rows droped from ticker 0322.HK.\n",
      "0 rows droped from ticker 0386.HK.\n",
      "0 rows droped from ticker 0669.HK.\n",
      "0 rows droped from ticker 0700.HK.\n",
      "4 rows droped from ticker 0762.HK.\n",
      "0 rows droped from ticker 0857.HK.\n",
      "2 rows droped from ticker 0868.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\0881.HK.csv doesn't exist. Skip\n",
      "0 rows droped from ticker 0883.HK.\n",
      "0 rows droped from ticker 0941.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\0968.HK.csv doesn't exist. Skip\n",
      "15 rows droped from ticker 0981.HK.\n",
      "0 rows droped from ticker 0992.HK.\n",
      "0 rows droped from ticker 1044.HK.\n",
      "0 rows droped from ticker 1088.HK.\n",
      "6 rows droped from ticker 1093.HK.\n",
      "1 rows droped from ticker 1177.HK.\n",
      "11 rows droped from ticker 1211.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1378.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1810.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1876.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1928.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1929.HK.csv doesn't exist. Skip\n",
      "0 rows droped from ticker 2020.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\2269.HK.csv doesn't exist. Skip\n",
      "1 rows droped from ticker 2313.HK.\n",
      "2 rows droped from ticker 2319.HK.\n",
      "3 rows droped from ticker 2331.HK.\n",
      "0 rows droped from ticker 2382.HK.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\3690.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\3692.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\6690.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\6862.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\9618.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\9633.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\9888.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\9988.HK.csv doesn't exist. Skip\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\9999.HK.csv doesn't exist. Skip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data(tic_list, RAW_DATA_WITHIN_RANGE_DIR, PREPROCESSED_HSI_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from https://www.slickcharts.com/dowjones\n",
    "DJI_components = '''\n",
    "#\tCompany\tSymbol\tWeight\t      Price\tChg\t% Chg\n",
    "1\tUnitedHealth Group Incorporated\tUNH\t9.201192\t   485.70\t8.82\t(1.85%)\n",
    "2\tGoldman Sachs Group Inc.\tGS\t7.223885\t   376.46\t2.06\t(0.55%)\n",
    "3\tHome Depot Inc.\tHD\t6.274593\t   320.79\t-4.41\t(-1.36%)\n",
    "4\tMicrosoft Corporation\tMSFT\t5.162454\t   270.42\t2.86\t(1.07%)\n",
    "5\tMcDonald's Corporation\tMCD\t5.158209\t   263.52\t-3.82\t(-1.43%)\n",
    "6\tCaterpillar Inc.\tCAT\t4.817081\t   248.87\t-0.79\t(-0.32%)\n",
    "7\tAmgen Inc.\tAMGN\t4.707874\t   243.00\t-1.00\t(-0.41%)\n",
    "8\tVisa Inc. Class A\tV\t4.463219\t   231.20\t-0.12\t(-0.05%)\n",
    "9\tBoeing Company\tBA\t4.143701\t   214.62\t-0.14\t(-0.07%)\n",
    "10\tHoneywell International Inc.\tHON\t3.964454\t   203.17\t-2.30\t(-1.12%)\n",
    "11\tTravelers Companies Inc.\tTRV\t3.646865\t   187.39\t-1.62\t(-0.86%)\n",
    "12\tAmerican Express Company\tAXP\t3.447939\t   179.22\t0.52\t(0.29%)\n",
    "13\tChevron Corporation\tCVX\t3.358991\t   170.21\t-3.88\t(-2.23%)\n",
    "14\tSalesforce Inc.\tCRM\t3.304773\t   172.44\t1.16\t(0.68%)\n",
    "15\tJohnson & Johnson\tJNJ\t3.152732\t   164.14\t0.74\t(0.45%)\n",
    "16\tApple Inc.\tAAPL\t2.983904\t   153.20\t-1.45\t(-0.94%)\n",
    "17\tJPMorgan Chase & Co.\tJPM\t2.771664\t   143.03\t-0.62\t(-0.43%)\n",
    "18\tWalmart Inc.\tWMT\t2.720148\t   139.28\t-1.70\t(-1.21%)\n",
    "19\tProcter & Gamble Company\tPG\t2.701625\t   139.12\t-0.90\t(-0.64%)\n",
    "20\tInternational Business Machines Corporation\tIBM\t2.620974\t   134.88\t-0.96\t(-0.71%)\n",
    "21\tNIKE Inc. Class B\tNKE\t2.418188\t   123.76\t-1.57\t(-1.25%)\n",
    "22\t3M Company\tMMM\t2.255535\t   116.00\t-0.90\t(-0.77%)\n",
    "23\tWalt Disney Company\tDIS\t2.153852\t   118.55\t6.92\t(6.20%)\n",
    "24\tMerck & Co. Inc.\tMRK\t2.03905\t   107.00\t1.32\t(1.25%)\n",
    "25\tDow Inc.\tDOW\t1.167514\t   59.81\t-0.70\t(-1.16%)\n",
    "26\tCoca-Cola Company\tKO\t1.159024\t   59.92\t-0.15\t(-0.25%)\n",
    "27\tCisco Systems Inc.\tCSCO\t0.923052\t   47.25\t-0.59\t(-1.23%)\n",
    "28\tVerizon Communications Inc.\tVZ\t0.782395\t   40.60\t0.05\t(0.12%)\n",
    "29\tWalgreens Boots Alliance Inc.\tWBA\t0.708882\t   36.74\t0.00\t(0.00%)\n",
    "30\tIntel Corporation\tINTC\t0.560507\t   28.47\t-0.58\t(-2.00%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(io.StringIO(DJI_components), sep='\\t')\n",
    "tic_list = df.Symbol.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download UNH.csv\n",
      "Download GS.csv\n",
      "Download HD.csv\n",
      "Download MSFT.csv\n",
      "Download MCD.csv\n",
      "Download CAT.csv\n",
      "Download AMGN.csv\n",
      "Download V.csv\n",
      "Download BA.csv\n",
      "Download HON.csv\n",
      "Download TRV.csv\n",
      "Download AXP.csv\n",
      "Download CVX.csv\n",
      "Download CRM.csv\n",
      "Download JNJ.csv\n",
      "Download AAPL.csv\n",
      "Download JPM.csv\n",
      "Download WMT.csv\n",
      "Download PG.csv\n",
      "Download IBM.csv\n",
      "Download NKE.csv\n",
      "Download MMM.csv\n",
      "Download DIS.csv\n",
      "Download MRK.csv\n",
      "Download DOW.csv\n",
      "Download KO.csv\n",
      "Download CSCO.csv\n",
      "Download VZ.csv\n",
      "Download WBA.csv\n",
      "Download INTC.csv\n"
     ]
    }
   ],
   "source": [
    "retry_list = download_ticker_with_yfince(tic_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_list = download_ticker_with_yfince(retry_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "V: from 2008-03-19 to 2023-02-08 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "DOW: from 2019-03-20 to 2023-02-08 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "download_ticker_with_yfince_within_range(tic_list, TRAIN_START_DAY, TRADE_END_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(tic_list, RAW_DATA_WITHIN_RANGE_DIR, PREPROCESSED_DJIA_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAX30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from \n",
    "DAX30_component = '''\n",
    "Symbol\tCompany Name\tLast Price\tChange\t% Change\tVolume\n",
    "HNR1.DE\tHannover Rück SE\t181.75\t0.00\t0.00%\t21,182\n",
    "1COV.DE\tCovestro AG\t42.57\t0.05\t+0.12%\t160,831\n",
    "AIR.DE\tAirbus SE\t113.60\t0.30\t+0.26%\t68,162\n",
    "BAS.DE\tBASF SE\t53.00\t0.14\t+0.26%\t406,746\n",
    "RWE.DE\tRWE Aktiengesellschaft\t40.49\t-0.12\t-0.30%\t197,185\n",
    "BAYN.DE\tBayer Aktiengesellschaft\t62.27\t-0.22\t-0.35%\t3,027,871\n",
    "FME.DE\tFresenius Medical Care AG & Co. KGaA\t36.68\t-0.13\t-0.35%\t88,328\n",
    "SY1.DE\tSymrise AG\t99.04\t-0.44\t-0.44%\t71,780\n",
    "ZAL.DE\tZalando SE\t42.32\t0.19\t+0.45%\t158,081\n",
    "DTE.DE\tDeutsche Telekom AG\t20.26\t0.11\t+0.55%\t1,418,280\n",
    "FRE.DE\tFresenius SE & Co. KGaA\t27.64\t-0.18\t-0.65%\t252,857\n",
    "LIN.DE\tLinde plc\t313.50\t-2.10\t-0.67%\t124,968\n",
    "BEI.DE\tBeiersdorf Aktiengesellschaft\t110.55\t0.75\t+0.68%\t35,654\n",
    "CON.DE\tContinental Aktiengesellschaft\t71.16\t0.50\t+0.71%\t44,916\n",
    "MTX.DE\tMTU Aero Engines AG\t227.00\t1.60\t+0.71%\t15,685\n",
    "ADS.DE\tadidas AG\t155.26\t1.26\t+0.82%\t65,209\n",
    "MRK.DE\tMERCK Kommanditgesellschaft auf Aktien\t186.55\t1.55\t+0.84%\t96,058\n",
    "SHL.DE\tSiemens Healthineers AG\t53.56\t-0.48\t-0.89%\t66,525\n",
    "DBK.DE\tDeutsche Bank Aktiengesellschaft\t11.78\t0.11\t+0.96%\t2,280,258\n",
    "VOW3.DE\tVolkswagen AG\t130.66\t1.36\t+1.05%\t145,762\n",
    "HEI.DE\tHeidelbergCement AG\t62.56\t0.68\t+1.10%\t63,760\n",
    "P911.DE\tDr. Ing. h.c. F. Porsche AG\t115.10\t1.30\t+1.14%\t80,614\n",
    "BMW.DE\tBayerische Motoren Werke Aktiengesellschaft\t98.70\t1.13\t+1.16%\t159,290\n",
    "ALV.DE\tAllianz SE\t223.35\t2.60\t+1.18%\t474,631\n",
    "DPW.DE\tDeutsche Post AG\t41.92\t0.51\t+1.24%\t453,072\n",
    "DTG.DE\tDaimler Truck Holding AG\t31.83\t0.52\t+1.64%\t327,260\n",
    "IFX.DE\tInfineon Technologies AG\t36.10\t0.59\t+1.66%\t450,803\n",
    "ENR.DE\tSiemens Energy AG\t18.92\t0.46\t+2.46%\t452,242\n",
    "DB1.DE\tDeutsche Börse AG\t171.50\t5.20\t+3.13%\t127,123\n",
    "SIE.DE\tSiemens Aktiengesellschaft\t151.40\t11.34\t+8.10%\t1,537,042\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(io.StringIO(DAX30_component), sep='\\t')\n",
    "tic_list = df.Symbol.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File datasets\\raw\\HNR1.DE.csv already exist. Skip\n",
      "File datasets\\raw\\1COV.DE.csv already exist. Skip\n",
      "File datasets\\raw\\AIR.DE.csv already exist. Skip\n",
      "File datasets\\raw\\BAS.DE.csv already exist. Skip\n",
      "File datasets\\raw\\RWE.DE.csv already exist. Skip\n",
      "File datasets\\raw\\BAYN.DE.csv already exist. Skip\n",
      "File datasets\\raw\\FME.DE.csv already exist. Skip\n",
      "File datasets\\raw\\SY1.DE.csv already exist. Skip\n",
      "File datasets\\raw\\ZAL.DE.csv already exist. Skip\n",
      "File datasets\\raw\\DTE.DE.csv already exist. Skip\n",
      "File datasets\\raw\\FRE.DE.csv already exist. Skip\n",
      "File datasets\\raw\\LIN.DE.csv already exist. Skip\n",
      "File datasets\\raw\\BEI.DE.csv already exist. Skip\n",
      "File datasets\\raw\\_CON.DE.csv already exist. Skip\n",
      "File datasets\\raw\\MTX.DE.csv already exist. Skip\n",
      "File datasets\\raw\\ADS.DE.csv already exist. Skip\n",
      "File datasets\\raw\\MRK.DE.csv already exist. Skip\n",
      "File datasets\\raw\\SHL.DE.csv already exist. Skip\n",
      "File datasets\\raw\\DBK.DE.csv already exist. Skip\n",
      "File datasets\\raw\\VOW3.DE.csv already exist. Skip\n",
      "File datasets\\raw\\HEI.DE.csv already exist. Skip\n",
      "File datasets\\raw\\P911.DE.csv already exist. Skip\n",
      "File datasets\\raw\\BMW.DE.csv already exist. Skip\n",
      "File datasets\\raw\\ALV.DE.csv already exist. Skip\n",
      "File datasets\\raw\\DPW.DE.csv already exist. Skip\n",
      "File datasets\\raw\\DTG.DE.csv already exist. Skip\n",
      "File datasets\\raw\\IFX.DE.csv already exist. Skip\n",
      "File datasets\\raw\\ENR.DE.csv already exist. Skip\n",
      "File datasets\\raw\\DB1.DE.csv already exist. Skip\n",
      "File datasets\\raw\\SIE.DE.csv already exist. Skip\n"
     ]
    }
   ],
   "source": [
    "retry_list = download_ticker_with_yfince(tic_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_list = download_ticker_with_yfince(retry_list, RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1COV.DE: from 2015-10-06 to 2023-02-09 out of range.\n",
      "ZAL.DE: from 2014-10-01 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "SHL.DE: from 2018-04-12 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "P911.DE: from 2022-09-30 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "DTG.DE: from 2021-12-10 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ENR.DE: from 2020-09-29 to 2023-02-09 out of range.\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "download_ticker_with_yfince_within_range(tic_list, TRAIN_START_DAY, TRADE_END_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 rows droped from ticker HNR1.DE.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\1COV.DE.csv doesn't exist. Skip\n",
      "5 rows droped from ticker AIR.DE.\n",
      "1 rows droped from ticker BAS.DE.\n",
      "3 rows droped from ticker RWE.DE.\n",
      "40 rows droped from ticker BAYN.DE.\n",
      "1 rows droped from ticker FME.DE.\n",
      "3 rows droped from ticker SY1.DE.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\ZAL.DE.csv doesn't exist. Skip\n",
      "3 rows droped from ticker DTE.DE.\n",
      "5 rows droped from ticker FRE.DE.\n",
      "2493 rows droped from ticker LIN.DE.\n",
      "LIN.DE: too many NaNs, discard.\n",
      "5 rows droped from ticker BEI.DE.\n",
      "3 rows droped from ticker CON.DE.\n",
      "4 rows droped from ticker MTX.DE.\n",
      "3 rows droped from ticker ADS.DE.\n",
      "5 rows droped from ticker MRK.DE.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\SHL.DE.csv doesn't exist. Skip\n",
      "3 rows droped from ticker DBK.DE.\n",
      "1 rows droped from ticker VOW3.DE.\n",
      "5 rows droped from ticker HEI.DE.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\P911.DE.csv doesn't exist. Skip\n",
      "3 rows droped from ticker BMW.DE.\n",
      "1 rows droped from ticker ALV.DE.\n",
      "4 rows droped from ticker DPW.DE.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\DTG.DE.csv doesn't exist. Skip\n",
      "3 rows droped from ticker IFX.DE.\n",
      "File datasets\\raw_2008-01-01_2022-12-31\\ENR.DE.csv doesn't exist. Skip\n",
      "5 rows droped from ticker DB1.DE.\n",
      "1 rows droped from ticker SIE.DE.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LIN.DE']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_data(tic_list, RAW_DATA_WITHIN_RANGE_DIR, PREPROCESSED_DAX30_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download index history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSE50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download SSE50, CSI300 from https://www.csindex.com.cn/#/indices/family/list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df = yf.download('^HSI', start=TRAIN_START_DAY, end=TRADE_END_DAY)\n",
    "df.to_csv(f'./datasets/HSI_{TRAIN_START_DAY}-{TRADE_END_DAY}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df = yf.download('^DJI', start=TRAIN_START_DAY, end=TRADE_END_DAY)\n",
    "df.to_csv(f'./datasets/DJI_{TRAIN_START_DAY}-{TRADE_END_DAY}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAX30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "df = yf.download('^GDAXI', start=TRAIN_START_DAY, end=TRADE_END_DAY)\n",
    "df.to_csv(f'./datasets/DAX30_{TRAIN_START_DAY}-{TRADE_END_DAY}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(PREPROCESSED_DATA_DIR, 'SSE50')\n",
    "# load data\n",
    "\n",
    "df_dict = {}\n",
    "_, _, files = next(os.walk(dataset_dir))\n",
    "for file in files:\n",
    "    processed_file_path = os.path.join(dataset_dir, file)   \n",
    "    df = pd.read_csv(processed_file_path, index_col=False)\n",
    "    tic = file.replace('.csv', '')\n",
    "    df_dict[tic] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TEST_START_DAY, TRADE_START_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_envs(n_tickers: int = 10, seed=None) -> Tuple[MultiStockTradingEnv, MultiStockTradingEnv, MultiStockTradingEnv]:\n",
    "    assert n_tickers <= len(df_dict_train)\n",
    "\n",
    "    env_list = list()\n",
    "    random.seed(seed)\n",
    "    tic_list = random.sample(df_dict_train.keys(), n_tickers)\n",
    "    print('Sampled tickers: ', tic_list)\n",
    "    for _df_dict in [df_dict_train, df_dict_test, df_dict_trade]:\n",
    "        if len(_df_dict) == 0:\n",
    "            env_list.append(None)\n",
    "        else:\n",
    "            _dfs = list()\n",
    "            for tic in tic_list:\n",
    "                _df = _df_dict[tic].copy()\n",
    "                _df['tic'] = tic\n",
    "                _dfs.append(_df)\n",
    "            _dfs = pd.concat(_dfs)\n",
    "            # drop dates that missing data\n",
    "            _dfs = _dfs.pivot_table(index=['date'], columns=['tic']).dropna().stack().reset_index()\n",
    "            _dfs.sort_values(['date', 'tic'], inplace=True)\n",
    "            _dfs.set_index(['date', 'tic'], inplace=True)\n",
    "            env_list.append(Monitor(MultiStockTradingEnv(_dfs)))\n",
    "    \n",
    "    return tuple(env_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(\n",
    "    model_name: str, \n",
    "    model_class: Type[BaseAlgorithm], \n",
    "    sample_param_func: Callable[[optuna.Trial], Tuple[Dict, int]],\n",
    "    ) -> Callable[[optuna.Trial], float]:\n",
    "    \n",
    "    def objective(trial: optuna.Trial):\n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "        model_path = os.path.join(model_path, f'trial_{trial.number}_best_model')\n",
    "        tb_log_path = os.path.join(TENSORBOARD_LOG_DIR, model_name)\n",
    "        check_and_make_directories([model_path, tb_log_path])\n",
    "\n",
    "        # Create model with sampled hyperparameters and \n",
    "        # train it with early stop callback    \n",
    "        hyperparameters, total_timesteps = sample_param_func(trial)\n",
    "        hyperparameters['tensorboard_log'] = '/root/tf-logs' # tb_log_path # or \n",
    "\n",
    "        env_train, env_test, _ =  get_envs()\n",
    "        model = model_class('MlpPolicy', env_train, **hyperparameters)\n",
    "\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(\n",
    "            max_no_improvement_evals=4, min_evals=2, verbose=VERBOSE)\n",
    "        eval_callback = EvalCallback(\n",
    "            env_test, \n",
    "            callback_after_eval=stop_train_callback,\n",
    "            n_eval_episodes=3,\n",
    "            eval_freq=10000,\n",
    "            best_model_save_path=model_path, \n",
    "            verbose=VERBOSE\n",
    "            )\n",
    "\n",
    "        # deal with gradient explosion\n",
    "        try:\n",
    "            model.learn(total_timesteps=total_timesteps, \n",
    "                tb_log_name=f'{model_name}_{trial.number}', callback=eval_callback)\n",
    "        except ValueError as e:\n",
    "            # Return when gradient exploded.\n",
    "            print(e)\n",
    "            return -9\n",
    "        except RuntimeError as e:\n",
    "            # Catch CUDA OOM\n",
    "            print(e)\n",
    "            return -9\n",
    "\n",
    "        # validation\n",
    "        mean_reward, _ = evaluate_policy(model, env_test, n_eval_episodes=3)\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(\n",
    "    model_name: str, \n",
    "    model_class: Type[BaseAlgorithm],\n",
    "    sample_param_func: Callable[[optuna.Trial], Any],\n",
    "    n_trials: int = 100, \n",
    "    callbacks: List[Callable] = None\n",
    "    ) -> optuna.Study:\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=None)\n",
    "    objective = objective_factory(model_name, model_class, sample_param_func)\n",
    "\n",
    "    study_name = f'{model_name}_study'\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f'sqlite:///{study_name}',\n",
    "        direction='maximize',\n",
    "        sampler=sampler,\n",
    "        pruner=optuna.pruners.HyperbandPruner(),\n",
    "        load_if_exists=True\n",
    "        )\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=n_trials,\n",
    "        callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to alleviate CUDA OOM\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = 'max_split_size_mb:64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_A2C = tune('A2C', A2C, \\\n",
    "#     sample_param_func=sample_a2c_param)\n",
    "\n",
    "# plot_optimization_history(study_A2C)\n",
    "# plot_param_importances(study_A2C)\n",
    "\n",
    "study_TQC = tune('TQC', TQC, \\\n",
    "    sample_param_func=sample_tqc_param)\n",
    "\n",
    "plot_optimization_history(study_TQC)\n",
    "plot_param_importances(study_TQC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [114, 123, 26, 103, 233] # 89, 41, 195, 202, 197\n",
    "N_REPEAT = len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE_path = './datasets/sse50.csv'\n",
    "HSI_path = './datasets/hsi.csv'\n",
    "DJI_path = './datasets/dji.csv'\n",
    "DAX30_path = './datasets/dax30.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/python3.9/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(SSE_path):\n",
    "    df_sse50 = pd.read_excel('./datasets/上证50_2008-2022.xlsx')\n",
    "\n",
    "    # clean\n",
    "    df_r = df_sse50[['日期Date', '开盘Open', '最高High', '最低Low', '收盘Close', '成交量（万手）Volume(M Shares)']].copy()\n",
    "    df_r.set_axis(['date', 'open', 'high', 'low', 'close', 'volume'], axis=1, inplace=True)\n",
    "    df_r['volume'] = df_r['volume'] * 10e4\n",
    "    df_r['date'] = pd.to_datetime(df_r['date'], format='%Y%m%d')\n",
    "\n",
    "    # split\n",
    "    start = pd.to_datetime(TRADE_START_DAY, format='%Y-%m-%d')\n",
    "    end = pd.to_datetime(TRADE_END_DAY, format='%Y-%m-%d')\n",
    "    df_r = df_r[(df_r['date'] >= start) & (df_r['date'] <= end)]\n",
    "\n",
    "    df_r.to_csv(SSE_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(in_dir, out_dir, start=TRADE_START_DAY, end=TRADE_END_DAY):\n",
    "    if os.path.exists(out_dir):\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(in_dir, index_col=False)\n",
    "    df.rename(columns={\n",
    "        'Date': 'date',\n",
    "        'Open': 'open',\n",
    "        'High': 'high',\n",
    "        'Low': 'low',\n",
    "        'Close': 'close',\n",
    "        'Adj Close' : 'adj_close',\n",
    "        'Volume' : 'volume'\n",
    "        }, inplace=True)\n",
    "\n",
    "    # split\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "    s = pd.to_datetime(start, format='%Y-%m-%d')\n",
    "    e = pd.to_datetime(end, format='%Y-%m-%d')\n",
    "    df = df[(df['date'] >= s) & (df['date'] <= e)]\n",
    "\n",
    "    assert df['close'].isna().sum() == 0\n",
    "\n",
    "    df.to_csv(out_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean('./datasets/HSI_2008-01-01-2022-12-31', HSI_path)\n",
    "clean('./datasets/DJI_2008-01-01-2022-12-31', DJI_path)\n",
    "clean('./datasets/DAX30_2008-01-01-2022-12-31', DAX30_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC VS TQCR VS TQCI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment\n",
    "df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TRADE_START_DAY, TRADE_START_DAY)\n",
    "\n",
    "model_info = []\n",
    "\n",
    "# SAC\n",
    "params = {\n",
    "    'learning_rate': 3 * 10 ** -5,\n",
    "    'buffer_size': 10 ** 5,\n",
    "    'learning_starts': 50,\n",
    "    'batch_size': 2 ** 5,\n",
    "    'train_freq': 2 ** 4,\n",
    "    'gradient_steps': 2 ** 3,\n",
    "    'target_update_interval': 10 ** 2,\n",
    "    'gamma': 1,\n",
    "    'policy_kwargs': {\n",
    "        'net_arch': [2 ** 7] * 5\n",
    "    }\n",
    "}\n",
    "model_info.append((SAC, params.copy()))\n",
    "\n",
    "# TQC\n",
    "params['top_quantiles_to_drop_per_net'] = 3\n",
    "params['policy_kwargs'] = {\n",
    "    'net_arch': [2 ** 7] * 5,\n",
    "    'n_quantiles': 2 ** 8,\n",
    "    'n_critics': 3\n",
    "}\n",
    "model_info.append((TQC, params.copy()))\n",
    "\n",
    "# TQCI\n",
    "params.pop('top_quantiles_to_drop_per_net')\n",
    "params['policy_kwargs'] = {\n",
    "    'net_arch': [2 ** 7] * 5,\n",
    "    'n_samples_critics': 64,\n",
    "    'n_samples_target_critcs': 64,\n",
    "    'cos_embedding_dims': 64,\n",
    "    'n_critics': 3\n",
    "}\n",
    "\n",
    "model_info.append((TQCI, params.copy()))\n",
    "\n",
    "total_timesteps = 2000 * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled tickers:  ['600519.SS', '601857.SS', '600104.SS', '600900.SS', '603799.SS', '600031.SS', '600276.SS', '601668.SS', '603288.SS', '601688.SS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_993281/3470458168.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  tic_list = random.sample(df_dict_train.keys(), n_tickers)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m check_and_make_directories(tb_log_path)\n\u001b[1;32m      7\u001b[0m params[\u001b[39m'\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m SEED[i]\n\u001b[0;32m----> 9\u001b[0m env_train, _, _ \u001b[39m=\u001b[39m get_envs(seed\u001b[39m=\u001b[39;49mSEED[i])\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m model_class(\n\u001b[1;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m     env_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     tensorboard_log\u001b[39m=\u001b[39mtb_log_path,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39mtotal_timesteps, tb_log_name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [61], line 25\u001b[0m, in \u001b[0;36mget_envs\u001b[0;34m(n_tickers, seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m         _dfs\u001b[39m.\u001b[39msort_values([\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtic\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m         _dfs\u001b[39m.\u001b[39mset_index([\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtic\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m         env_list\u001b[39m.\u001b[39mappend(Monitor(MultiStockTradingEnv(_dfs)))\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(env_list)\n",
      "File \u001b[0;32m~/Experiment/StockTrading/environment/MultiStockTradingEnv.py:52\u001b[0m, in \u001b[0;36mMultiStockTradingEnv.__init__\u001b[0;34m(self, df, transaction_cost, stack_frame)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mBox(low\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shape\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tickers \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,)) \n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mBox(low\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, high\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39minf, shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_shape)\n\u001b[0;32m---> 52\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset()\n",
      "File \u001b[0;32m~/Experiment/StockTrading/environment/MultiStockTradingEnv.py:65\u001b[0m, in \u001b[0;36mMultiStockTradingEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mportfolio_memory \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mportfolio]\n\u001b[1;32m     64\u001b[0m \u001b[39m# update state\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_data()\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_state()\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/Experiment/StockTrading/environment/MultiStockTradingEnv.py:95\u001b[0m, in \u001b[0;36mMultiStockTradingEnv._update_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_data\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39m# bound checking\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdate\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     96\u001b[0m     cur_dates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdate[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack_frame\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mloc[cur_dates]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(N_REPEAT):\n",
    "    for model_class, params in model_info:\n",
    "        model_name = model_class.__name__\n",
    "        tb_log_path = os.path.join(TENSORBOARD_LOG_DIR, f'{model_name}_SSE50')\n",
    "        check_and_make_directories(tb_log_path)\n",
    "\n",
    "        params['seed'] = SEED[i]\n",
    "\n",
    "        env_train, _, _ = get_envs(seed=SEED[i])\n",
    "\n",
    "        model = model_class(\n",
    "            'MlpPolicy',\n",
    "            env_train,\n",
    "            **params,\n",
    "            verbose=1,\n",
    "            tensorboard_log=tb_log_path,\n",
    "        )\n",
    "        model.learn(total_timesteps=total_timesteps, tb_log_name=f'train_{model_name}_{i}')\n",
    "        \n",
    "        model_path = os.path.join(MODEL_TRAINED_DIR, f'{model_name}_{i}.pth')\n",
    "        model.save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TRADE_START_DAY, TRADE_START_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SAC, TQC, TQCI]\n",
    "\n",
    "result = {}\n",
    "for model_class in models:\n",
    "    model_name = model_class.__name__\n",
    "    for i in range(N_REPEAT):\n",
    "        _, _, env_trade = get_envs(seed=SEED[i])\n",
    "        tics = env_trade.df.reset_index().tic.unique()\n",
    "        dates = env_trade.df.reset_index().date.unique()\n",
    "\n",
    "        model_path = os.path.join(MODEL_TRAINED_DIR, f'{model_name}_{i}.pth')\n",
    "        model = model_class.load(model_path)\n",
    "\n",
    "        list_asset, actions, rewards = simulate_trading(env_trade, model)\n",
    "        result[model_name].append((tics, dates, list_asset))\n",
    "\n",
    "with open('assets_SSE50.pkl', 'wb') as fout:\n",
    "    pickle.dump(result, fout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets_SSE50.pkl', 'rb') as fin:\n",
    "    result = pickle.load(fin)\n",
    "\n",
    "df_sse50 = pd.read_csv(SSE_path)\n",
    "\n",
    "df_assets = pd.DataFrame()\n",
    "for model_name, tuples in result.items():\n",
    "    print(model_name)\n",
    "   \n",
    "    for tup in tuples:\n",
    "        tics, dates, asset = tup\n",
    "        df_t = pd.DataFrame(data={'date': dates, 'asset': asset})\n",
    "        df_t['model'] = model_name\n",
    "        df_t['tics'] = tics\n",
    "        df_assets = pd.concat([df_assets, df_t])\n",
    "\n",
    "    tics, dates, asset = tup\n",
    "    print(tics)\n",
    "    plot_asset = pd.Series(data=asset, index=dates)\n",
    "    baseline_asset = df_sse50['close'][:-len(plot_asset)]\n",
    "    backtest_plot(plot_asset, baseline_asset)\n",
    "\n",
    "print('-' * 80)\n",
    "sns.lineplot(df_assets, x='date', y='asset', hue='model', errorbar='sd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE50VS HSI VS DJI VS DAX30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQCI\n",
    "params = {\n",
    "    'learning_rate': 3 * 10 ** -5,\n",
    "    'buffer_size': 10 ** 5,\n",
    "    'learning_starts': 50,\n",
    "    'batch_size': 2 ** 5,\n",
    "    'train_freq': 2 ** 4,\n",
    "    'gradient_steps': 2 ** 3,\n",
    "    'target_update_interval': 10 ** 2,\n",
    "    'gamma': 1,\n",
    "    'policy_kwargs': {\n",
    "        'net_arch': [2 ** 7] * 5,\n",
    "        'n_samples_critics': 64,\n",
    "        'n_samples_target_critcs': 64,\n",
    "        'cos_embedding_dims': 64,\n",
    "        'n_critics': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "total_timesteps = 2000 * 200\n",
    "\n",
    "for i in range(N_REPEAT):\n",
    "    for index_name in ['SSE50', 'HSI', 'DJIA', 'DAX30']:\n",
    "\n",
    "        # load data\n",
    "        dataset_dir = os.path.join(PREPROCESSED_DATA_DIR, index_name)\n",
    "        df_dict = {}\n",
    "        _, _, files = next(os.walk(dataset_dir))\n",
    "        for file in files:\n",
    "            processed_file_path = os.path.join(dataset_dir, file)   \n",
    "            df = pd.read_csv(processed_file_path, index_col=False)\n",
    "            tic = file.replace('.csv', '')\n",
    "            df_dict[tic] = df.copy()\n",
    "        \n",
    "        df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TRADE_START_DAY, TRADE_START_DAY)\n",
    "\n",
    "        env_train, _, _ = get_envs(seed=SEED[i])\n",
    "\n",
    "        tb_log_path = os.path.join(TENSORBOARD_LOG_DIR, f'TQCI_{index_name}')\n",
    "        check_and_make_directories(tb_log_path)\n",
    "\n",
    "        model = TQCI(\n",
    "            'MlpPolicy',\n",
    "            env_train,\n",
    "            **params,\n",
    "            verbose=1,\n",
    "            tensorboard_log=tb_log_path,\n",
    "            seed=SEED[i]\n",
    "        )\n",
    "        model.learn(total_timesteps=total_timesteps, tb_log_name=f'train_tqci_on_{index_name}')\n",
    "        \n",
    "        model_path = os.path.join(MODEL_TRAINED_DIR, f'tqci_on_{index_name}_{i}.pth')\n",
    "        model.save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for i in range(N_REPEAT):\n",
    "    for index_name in ['SSE50', 'HSI', 'DJIA', 'DAX30']:\n",
    "\n",
    "        # load data\n",
    "        dataset_dir = os.path.join(PREPROCESSED_DATA_DIR, index_name)\n",
    "        df_dict = {}\n",
    "        _, _, files = next(os.walk(dataset_dir))\n",
    "        for file in files:\n",
    "            processed_file_path = os.path.join(dataset_dir, file)   \n",
    "            df = pd.read_csv(processed_file_path, index_col=False)\n",
    "            tic = file.replace('.csv', '')\n",
    "            df_dict[tic] = df.copy()\n",
    "        \n",
    "        df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TRADE_START_DAY, TRADE_START_DAY)\n",
    "\n",
    "        _, _, env_trade = get_envs(seed=SEED[i])\n",
    "\n",
    "        tics = env_trade.df.reset_index().tic.unique()\n",
    "        dates = env_trade.df.reset_index().date.unique()\n",
    "\n",
    "\n",
    "        model_path = os.path.join(MODEL_TRAINED_DIR, f'tqci_on_{index_name}_{i}.pth')\n",
    "        model = TQCI.load(model_path)\n",
    "\n",
    "        list_asset, actions, rewards = simulate_trading(env_trade, model)\n",
    "        result[index_name].append((tics, dates, list_asset))\n",
    "\n",
    "with open('assets_indexes.pkl', 'wb') as fout:\n",
    "    pickle.dump(result, fout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets_indexes.pkl', 'rb') as fin:\n",
    "    result = pickle.load(fin)\n",
    "\n",
    "df_sse50 = pd.read_csv(SSE_path)\n",
    "df_hsi = pd.read_csv(HSI_path)\n",
    "df_dji = pd.read_csv(DJI_path)\n",
    "df_dax30 = pd.read_csv(DAX30_path)\n",
    "baselines = {\n",
    "    'SSE50': df_sse50,\n",
    "    'HSI': df_hsi,\n",
    "    'DJI': df_dji,\n",
    "    'DAX30': df_dax30,\n",
    "}\n",
    "\n",
    "for index_name, tuples in result.items():\n",
    "    print(index_name)\n",
    "\n",
    "    df_assets = pd.DataFrame()\n",
    "    for tup in tuples:\n",
    "        tics, dates, asset = tup\n",
    "        df_t = pd.DataFrame(data={'date': dates, 'asset': asset})\n",
    "        df_t['index_name'] = index_name\n",
    "        df_t['tics'] = tics\n",
    "        df_assets = pd.concat([df_assets, df_t])\n",
    "\n",
    "    tics, dates, asset = tup\n",
    "    plot_asset = pd.Series(data=asset, index=dates)\n",
    "\n",
    "    baseline = baselines[index_name]\n",
    "    baseline_asset = baseline['close'][:-len(plot_asset)]\n",
    "    backtest_plot(plot_asset, baseline_asset)\n",
    "\n",
    "    sns.lineplot(df_assets, x='day', y='asset', errorbar='sd')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk distortion measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPW_factory(eta: float) -> Callable[[th.Tensor], th.Tensor]:\n",
    "    def CPW(taus: th.Tensor) -> th.Tensor:\n",
    "        taus = (taus ** eta) / ((taus ** eta + (1 - taus) ** eta) ** (1 / eta))\n",
    "        return taus\n",
    "    return CPW\n",
    "\n",
    "def Wang_factory(tau: float, eta: float) -> Callable[[float], float]:\n",
    "    def Wang(taus: float) -> float:\n",
    "        taus = norm.cdf(norm.ppf(tau) + eta)\n",
    "        return taus\n",
    "    return Wang\n",
    "\n",
    "def CVaR_factory(eta: float) -> Callable[[th.Tensor], th.Tensor]:\n",
    "    def CVaR(taus: th.Tensor) -> th.Tensor:\n",
    "        taus = taus * eta\n",
    "        return taus\n",
    "    return CVaR\n",
    "\n",
    "def Norm_factory(eta: int) -> Callable[[th.Tensor], th.Tensor]:\n",
    "    def Norm(taus: th.Tensor) -> th.Tensor:\n",
    "        taus = taus.repeat([1] * len(taus.shape) + [3]).uniform_(0, 1).mean(axis=-1)\n",
    "        return taus\n",
    "    return Norm\n",
    "\n",
    "def Pow_factory(eta: float) -> Callable[[th.Tensor], th.Tensor]:\n",
    "    def Pow(taus: th.Tensor) -> th.Tensor:\n",
    "        if eta >= 0:\n",
    "            taus = taus ** (1 / (1 + eta))\n",
    "        else:\n",
    "            taus = 1 - (1 - taus) ** (1 / (1 - eta))\n",
    "        return taus\n",
    "    return Pow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdms = {}\n",
    "rdms['CPW_0.71'] = CPW_factory(0.71)\n",
    "rdms['Wang_-0.75'] = Wang_factory(-0.75)\n",
    "rdms['CVaR_0.25'] = CVaR_factory(0.25)\n",
    "rdms['CVaR_0.4'] = CVaR_factory(0.4)\n",
    "rdms['Norm_3']= Norm_factory(3)\n",
    "rdms['Pow_-2'] = Pow_factory(-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset_dir = os.path.join(PREPROCESSED_DATA_DIR, 'SSE50')\n",
    "df_dict = {}\n",
    "_, _, files = next(os.walk(dataset_dir))\n",
    "for file in files:\n",
    "    processed_file_path = os.path.join(dataset_dir, file)   \n",
    "    df = pd.read_csv(processed_file_path, index_col=False)\n",
    "    tic = file.replace('.csv', '')\n",
    "    df_dict[tic] = df.copy()\n",
    "\n",
    "df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TRADE_START_DAY, TRADE_START_DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TQCI\n",
    "params = {\n",
    "    'learning_rate': 3 * 10 ** -5,\n",
    "    'buffer_size': 10 ** 5,\n",
    "    'learning_starts': 50,\n",
    "    'batch_size': 2 ** 5,\n",
    "    'train_freq': 2 ** 4,\n",
    "    'gradient_steps': 2 ** 3,\n",
    "    'target_update_interval': 10 ** 2,\n",
    "    'gamma': 1,\n",
    "    'policy_kwargs': {\n",
    "        'net_arch': [2 ** 7] * 5,\n",
    "        'n_samples_critics': 64,\n",
    "        'n_samples_target_critcs': 64,\n",
    "        'cos_embedding_dims': 64,\n",
    "        'n_critics': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "total_timesteps = 2000 * 200\n",
    "\n",
    "for i in range(N_REPEAT):\n",
    "    for name, rdm in rdms.items():\n",
    "\n",
    "        # load data\n",
    "        dataset_dir = os.path.join(PREPROCESSED_DATA_DIR, 'SSE50')\n",
    "        df_dict = {}\n",
    "        _, _, files = next(os.walk(dataset_dir))\n",
    "        for file in files:\n",
    "            processed_file_path = os.path.join(dataset_dir, file)   \n",
    "            df = pd.read_csv(processed_file_path, index_col=False)\n",
    "            tic = file.replace('.csv', '')\n",
    "            df_dict[tic] = df.copy()\n",
    "        \n",
    "        df_dict_train, df_dict_test, df_dict_trade = split_data(df_dict, TRADE_START_DAY, TRADE_START_DAY)\n",
    "\n",
    "        env_train, _, _ = get_envs(seed=SEED[i])\n",
    "        \n",
    "\n",
    "        tb_log_path = os.path.join(TENSORBOARD_LOG_DIR, f'TQCI_SSE50_{name}')\n",
    "        check_and_make_directories(tb_log_path)\n",
    "\n",
    "        params['policy_kwargs']['risk_distortion_measures'] = rdm\n",
    "        model = TQCI(\n",
    "            'MlpPolicy',\n",
    "            env_train,\n",
    "            **params,\n",
    "            verbose=1,\n",
    "            tensorboard_log=tb_log_path,\n",
    "            seed=SEED[i]\n",
    "        )\n",
    "        model.learn(total_timesteps=total_timesteps, tb_log_name=f'train_tqci_with_{name}')\n",
    "        \n",
    "        model_path = os.path.join(MODEL_TRAINED_DIR, f'tqci_with_{name}_{i}.pth')\n",
    "        model.save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for i in range(N_REPEAT):\n",
    "    for name, rdm in rdms.items():\n",
    "\n",
    "        _, _, env_trade = get_envs(seed=SEED[i])\n",
    "\n",
    "        model_path = os.path.join(MODEL_TRAINED_DIR, f'tqci_with_{name}_{i}.pth')\n",
    "        model = TQCI.load(model_path)\n",
    "\n",
    "        list_asset, actions, rewards = simulate_trading(env_trade, model)\n",
    "        result[name].append(list_asset)\n",
    "\n",
    "with open('assets_risk.pkl', 'wb') as fout:\n",
    "    pickle.dump(result, fout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets_risk.pkl', 'rb') as fin:\n",
    "    result = pickle.load(fin)\n",
    "\n",
    "df_sse50 = pd.read_csv(SSE_path)\n",
    "\n",
    "df_assets = pd.DataFrame()\n",
    "for rdm_name, tuples in result.items():\n",
    "    print(rdm_name)\n",
    "   \n",
    "    for tup in tuples:\n",
    "        tics, dates, asset = tup\n",
    "        df_t = pd.DataFrame(data={'date': dates, 'asset': asset})\n",
    "        df_t['rdm_name'] = rdm_name\n",
    "        df_t['tics'] = tics\n",
    "        df_assets = pd.concat([df_assets, df_t])\n",
    "\n",
    "    tics, dates, asset = tup\n",
    "    print(tics)\n",
    "    plot_asset = pd.Series(data=asset, index=dates)\n",
    "    baseline_asset = df_sse50['close'][:-len(plot_asset)]\n",
    "    backtest_plot(plot_asset, baseline_asset)\n",
    "\n",
    "print('-' * 80)\n",
    "sns.lineplot(df_assets, x='date', y='asset', hue='rdm_name', errorbar='sd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "606c65d0c67f4fb4d4dcc32359b236efe9c56120e1c34efc12f6c8d1f87ab33f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
